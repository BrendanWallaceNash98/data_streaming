:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-70593044-dab1-4a07-8887-400482a95a3e;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 in central
	found org.apache.kafka#kafka-clients;2.6.0 in central
	found com.github.luben#zstd-jni;1.4.8-1 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.8.2 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
	found com.datastax.spark#spark-cassandra-connector_2.12;3.1.0 in central
	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0 in central
	found com.datastax.oss#java-driver-core-shaded;4.12.0 in central
	found com.datastax.oss#native-protocol;1.5.0 in central
	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
	found com.typesafe#config;1.4.1 in central
	found io.dropwizard.metrics#metrics-core;4.1.18 in central
	found org.hdrhistogram#HdrHistogram;2.1.12 in central
	found org.reactivestreams#reactive-streams;1.0.3 in central
	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
	found com.google.code.findbugs#jsr305;3.0.2 in central
	found com.datastax.oss#java-driver-mapper-runtime;4.12.0 in central
	found com.datastax.oss#java-driver-query-builder;4.12.0 in central
	found org.apache.commons#commons-lang3;3.10 in central
	found com.thoughtworks.paranamer#paranamer;2.8 in central
	found org.scala-lang#scala-reflect;2.12.11 in central
:: resolution report :: resolve 333ms :: artifacts dl 8ms
	:: modules in use:
	com.datastax.oss#java-driver-core-shaded;4.12.0 from central in [default]
	com.datastax.oss#java-driver-mapper-runtime;4.12.0 from central in [default]
	com.datastax.oss#java-driver-query-builder;4.12.0 from central in [default]
	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
	com.datastax.oss#native-protocol;1.5.0 from central in [default]
	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0 from central in [default]
	com.datastax.spark#spark-cassandra-connector_2.12;3.1.0 from central in [default]
	com.github.luben#zstd-jni;1.4.8-1 from central in [default]
	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
	com.typesafe#config;1.4.1 from central in [default]
	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
	org.apache.commons#commons-lang3;3.10 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.6.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 from central in [default]
	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
	org.scala-lang#scala-reflect;2.12.11 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
	:: evicted modules:
	org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;1.7.30] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   27  |   0   |   0   |   1   ||   26  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-70593044-dab1-4a07-8887-400482a95a3e
	confs: [default]
	0 artifacts copied, 26 already retrieved (0kB/6ms)
24/09/12 12:05:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-12 12:05:58,094 - INFO - Starting Spark Streaming application
2024-09-12 12:05:58,096 - DEBUG - GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
2024-09-12 12:05:58,096 - DEBUG - Command to send: A
c4e9aeb59a7b2ab0cea07f85f99307029b1ae69ea8001d75b4f6eca5e2209e68

2024-09-12 12:05:58,101 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,101 - DEBUG - Command to send: j
i
rj
org.apache.spark.SparkConf
e

2024-09-12 12:05:58,101 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,101 - DEBUG - Command to send: j
i
rj
org.apache.spark.api.java.*
e

2024-09-12 12:05:58,102 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,102 - DEBUG - Command to send: j
i
rj
org.apache.spark.api.python.*
e

2024-09-12 12:05:58,102 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,102 - DEBUG - Command to send: j
i
rj
org.apache.spark.ml.python.*
e

2024-09-12 12:05:58,102 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,102 - DEBUG - Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

2024-09-12 12:05:58,102 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,102 - DEBUG - Command to send: j
i
rj
org.apache.spark.resource.*
e

2024-09-12 12:05:58,103 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,103 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.*
e

2024-09-12 12:05:58,103 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,103 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

2024-09-12 12:05:58,103 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,103 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

2024-09-12 12:05:58,104 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,104 - DEBUG - Command to send: j
i
rj
scala.Tuple2
e

2024-09-12 12:05:58,104 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,104 - DEBUG - Command to send: r
u
SparkConf
rj
e

2024-09-12 12:05:58,108 - DEBUG - Answer received: !ycorg.apache.spark.SparkConf
2024-09-12 12:05:58,108 - DEBUG - Command to send: i
org.apache.spark.SparkConf
bTrue
e

2024-09-12 12:05:58,111 - DEBUG - Answer received: !yro0
2024-09-12 12:05:58,111 - DEBUG - Command to send: c
o0
set
sspark.app.name
sSparkDataStreaming
e

2024-09-12 12:05:58,114 - DEBUG - Answer received: !yro1
2024-09-12 12:05:58,114 - DEBUG - Command to send: c
o0
set
sspark.jars.packages
scom.datastax.spark:spark-cassandra-connector_2.12:3.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1
e

2024-09-12 12:05:58,114 - DEBUG - Answer received: !yro2
2024-09-12 12:05:58,114 - DEBUG - Command to send: c
o0
set
sspark.cassandra.connection.host
scassandra_db
e

2024-09-12 12:05:58,114 - DEBUG - Answer received: !yro3
2024-09-12 12:05:58,115 - DEBUG - Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

2024-09-12 12:05:58,115 - DEBUG - Answer received: !ybfalse
2024-09-12 12:05:58,115 - DEBUG - Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

2024-09-12 12:05:58,115 - DEBUG - Answer received: !yro4
2024-09-12 12:05:58,115 - DEBUG - Command to send: c
o0
contains
sspark.rdd.compress
e

2024-09-12 12:05:58,116 - DEBUG - Answer received: !ybfalse
2024-09-12 12:05:58,116 - DEBUG - Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

2024-09-12 12:05:58,116 - DEBUG - Answer received: !yro5
2024-09-12 12:05:58,116 - DEBUG - Command to send: c
o0
contains
sspark.master
e

2024-09-12 12:05:58,116 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:58,116 - DEBUG - Command to send: c
o0
contains
sspark.app.name
e

2024-09-12 12:05:58,117 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:58,117 - DEBUG - Command to send: c
o0
contains
sspark.master
e

2024-09-12 12:05:58,117 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:58,117 - DEBUG - Command to send: c
o0
get
sspark.master
e

2024-09-12 12:05:58,118 - DEBUG - Answer received: !ysspark://spark-master:7077
2024-09-12 12:05:58,118 - DEBUG - Command to send: c
o0
contains
sspark.app.name
e

2024-09-12 12:05:58,118 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:58,118 - DEBUG - Command to send: c
o0
get
sspark.app.name
e

2024-09-12 12:05:58,118 - DEBUG - Answer received: !ysSparkDataStreaming
2024-09-12 12:05:58,118 - DEBUG - Command to send: c
o0
contains
sspark.home
e

2024-09-12 12:05:58,119 - DEBUG - Answer received: !ybfalse
2024-09-12 12:05:58,119 - DEBUG - Command to send: c
o0
getAll
e

2024-09-12 12:05:58,119 - DEBUG - Answer received: !yto6
2024-09-12 12:05:58,119 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,119 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,119 - DEBUG - Command to send: a
g
o6
i0
e

2024-09-12 12:05:58,120 - DEBUG - Answer received: !yro7
2024-09-12 12:05:58,120 - DEBUG - Command to send: c
o7
_1
e

2024-09-12 12:05:58,120 - DEBUG - Answer received: !ysspark.app.name
2024-09-12 12:05:58,120 - DEBUG - Command to send: c
o7
_2
e

2024-09-12 12:05:58,120 - DEBUG - Answer received: !ysSparkDataStreaming
2024-09-12 12:05:58,120 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,121 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,121 - DEBUG - Command to send: a
g
o6
i1
e

2024-09-12 12:05:58,121 - DEBUG - Answer received: !yro8
2024-09-12 12:05:58,121 - DEBUG - Command to send: c
o8
_1
e

2024-09-12 12:05:58,121 - DEBUG - Answer received: !ysspark.cassandra.connection.host
2024-09-12 12:05:58,121 - DEBUG - Command to send: c
o8
_2
e

2024-09-12 12:05:58,121 - DEBUG - Answer received: !yscassandra_db
2024-09-12 12:05:58,121 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,122 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,122 - DEBUG - Command to send: a
g
o6
i2
e

2024-09-12 12:05:58,122 - DEBUG - Answer received: !yro9
2024-09-12 12:05:58,122 - DEBUG - Command to send: c
o9
_1
e

2024-09-12 12:05:58,122 - DEBUG - Answer received: !ysspark.files
2024-09-12 12:05:58,122 - DEBUG - Command to send: c
o9
_2
e

2024-09-12 12:05:58,122 - DEBUG - Answer received: !ysfile:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar
2024-09-12 12:05:58,122 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,123 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,123 - DEBUG - Command to send: a
g
o6
i3
e

2024-09-12 12:05:58,123 - DEBUG - Answer received: !yro10
2024-09-12 12:05:58,123 - DEBUG - Command to send: c
o10
_1
e

2024-09-12 12:05:58,123 - DEBUG - Answer received: !ysspark.jars.packages
2024-09-12 12:05:58,123 - DEBUG - Command to send: c
o10
_2
e

2024-09-12 12:05:58,123 - DEBUG - Answer received: !yscom.datastax.spark:spark-cassandra-connector_2.12:3.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1
2024-09-12 12:05:58,123 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,124 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,124 - DEBUG - Command to send: a
g
o6
i4
e

2024-09-12 12:05:58,124 - DEBUG - Answer received: !yro11
2024-09-12 12:05:58,124 - DEBUG - Command to send: c
o11
_1
e

2024-09-12 12:05:58,124 - DEBUG - Answer received: !ysspark.rdd.compress
2024-09-12 12:05:58,124 - DEBUG - Command to send: c
o11
_2
e

2024-09-12 12:05:58,124 - DEBUG - Answer received: !ysTrue
2024-09-12 12:05:58,124 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,124 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,124 - DEBUG - Command to send: a
g
o6
i5
e

2024-09-12 12:05:58,125 - DEBUG - Answer received: !yro12
2024-09-12 12:05:58,125 - DEBUG - Command to send: c
o12
_1
e

2024-09-12 12:05:58,125 - DEBUG - Answer received: !ysspark.master
2024-09-12 12:05:58,125 - DEBUG - Command to send: c
o12
_2
e

2024-09-12 12:05:58,125 - DEBUG - Answer received: !ysspark://spark-master:7077
2024-09-12 12:05:58,125 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,125 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,125 - DEBUG - Command to send: a
g
o6
i6
e

2024-09-12 12:05:58,126 - DEBUG - Answer received: !yro13
2024-09-12 12:05:58,126 - DEBUG - Command to send: c
o13
_1
e

2024-09-12 12:05:58,126 - DEBUG - Answer received: !ysspark.serializer.objectStreamReset
2024-09-12 12:05:58,126 - DEBUG - Command to send: c
o13
_2
e

2024-09-12 12:05:58,126 - DEBUG - Answer received: !ys100
2024-09-12 12:05:58,126 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,126 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,126 - DEBUG - Command to send: a
g
o6
i7
e

2024-09-12 12:05:58,127 - DEBUG - Answer received: !yro14
2024-09-12 12:05:58,127 - DEBUG - Command to send: c
o14
_1
e

2024-09-12 12:05:58,127 - DEBUG - Answer received: !ysspark.jars
2024-09-12 12:05:58,127 - DEBUG - Command to send: c
o14
_2
e

2024-09-12 12:05:58,127 - DEBUG - Answer received: !ysfile:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar
2024-09-12 12:05:58,127 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,127 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,128 - DEBUG - Command to send: a
g
o6
i8
e

2024-09-12 12:05:58,128 - DEBUG - Answer received: !yro15
2024-09-12 12:05:58,128 - DEBUG - Command to send: c
o15
_1
e

2024-09-12 12:05:58,128 - DEBUG - Answer received: !ysspark.submit.deployMode
2024-09-12 12:05:58,128 - DEBUG - Command to send: c
o15
_2
e

2024-09-12 12:05:58,128 - DEBUG - Answer received: !ysclient
2024-09-12 12:05:58,128 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,128 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,128 - DEBUG - Command to send: a
g
o6
i9
e

2024-09-12 12:05:58,129 - DEBUG - Answer received: !yro16
2024-09-12 12:05:58,129 - DEBUG - Command to send: c
o16
_1
e

2024-09-12 12:05:58,129 - DEBUG - Answer received: !ysspark.repl.local.jars
2024-09-12 12:05:58,129 - DEBUG - Command to send: c
o16
_2
e

2024-09-12 12:05:58,129 - DEBUG - Answer received: !ysfile:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar
2024-09-12 12:05:58,129 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,130 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,130 - DEBUG - Command to send: a
g
o6
i10
e

2024-09-12 12:05:58,130 - DEBUG - Answer received: !yro17
2024-09-12 12:05:58,130 - DEBUG - Command to send: c
o17
_1
e

2024-09-12 12:05:58,130 - DEBUG - Answer received: !ysspark.driver.extraJavaOptions
2024-09-12 12:05:58,130 - DEBUG - Command to send: c
o17
_2
e

2024-09-12 12:05:58,130 - DEBUG - Answer received: !ys--add-exports java.base/sun.nio.ch=ALL-UNNAMED
2024-09-12 12:05:58,130 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,131 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,131 - DEBUG - Command to send: a
g
o6
i11
e

2024-09-12 12:05:58,131 - DEBUG - Answer received: !yro18
2024-09-12 12:05:58,131 - DEBUG - Command to send: c
o18
_1
e

2024-09-12 12:05:58,131 - DEBUG - Answer received: !ysspark.app.submitTime
2024-09-12 12:05:58,131 - DEBUG - Command to send: c
o18
_2
e

2024-09-12 12:05:58,131 - DEBUG - Answer received: !ys1726142757665
2024-09-12 12:05:58,132 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,132 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,132 - DEBUG - Command to send: a
g
o6
i12
e

2024-09-12 12:05:58,132 - DEBUG - Answer received: !yro19
2024-09-12 12:05:58,132 - DEBUG - Command to send: c
o19
_1
e

2024-09-12 12:05:58,132 - DEBUG - Answer received: !ysspark.submit.pyFiles
2024-09-12 12:05:58,132 - DEBUG - Command to send: c
o19
_2
e

2024-09-12 12:05:58,132 - DEBUG - Answer received: !ys/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,/opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar,/opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,/opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar,/opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,/opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,/opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,/opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar,/opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,/opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,/opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,/opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,/opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,/opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar
2024-09-12 12:05:58,132 - DEBUG - Command to send: a
e
o6
e

2024-09-12 12:05:58,133 - DEBUG - Answer received: !yi13
2024-09-12 12:05:58,133 - DEBUG - Command to send: r
u
JavaSparkContext
rj
e

2024-09-12 12:05:58,138 - DEBUG - Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
2024-09-12 12:05:58,138 - DEBUG - Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

24/09/12 12:05:58 INFO SparkContext: Running Spark version 3.5.2
24/09/12 12:05:58 INFO SparkContext: OS info Linux, 6.4.16-linuxkit, aarch64
24/09/12 12:05:58 INFO SparkContext: Java version 17.0.12
24/09/12 12:05:58 INFO ResourceUtils: ==============================================================
24/09/12 12:05:58 INFO ResourceUtils: No custom resources configured for spark.driver.
24/09/12 12:05:58 INFO ResourceUtils: ==============================================================
24/09/12 12:05:58 INFO SparkContext: Submitted application: SparkDataStreaming
24/09/12 12:05:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/09/12 12:05:58 INFO ResourceProfile: Limiting resource is cpu
24/09/12 12:05:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/09/12 12:05:58 INFO SecurityManager: Changing view acls to: spark
24/09/12 12:05:58 INFO SecurityManager: Changing modify acls to: spark
24/09/12 12:05:58 INFO SecurityManager: Changing view acls groups to: 
24/09/12 12:05:58 INFO SecurityManager: Changing modify acls groups to: 
24/09/12 12:05:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
24/09/12 12:05:58 INFO Utils: Successfully started service 'sparkDriver' on port 38537.
24/09/12 12:05:58 INFO SparkEnv: Registering MapOutputTracker
24/09/12 12:05:58 INFO SparkEnv: Registering BlockManagerMaster
24/09/12 12:05:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/09/12 12:05:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/09/12 12:05:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/09/12 12:05:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-35fe296b-19ca-4b73-b093-e121c0d85cae
24/09/12 12:05:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/09/12 12:05:58 INFO SparkEnv: Registering OutputCommitCoordinator
24/09/12 12:05:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/09/12 12:05:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar at spark://c8faee0cb5df:38537/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar at spark://c8faee0cb5df:38537/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar at spark://c8faee0cb5df:38537/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at spark://c8faee0cb5df:38537/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://c8faee0cb5df:38537/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://c8faee0cb5df:38537/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at spark://c8faee0cb5df:38537/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://c8faee0cb5df:38537/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at spark://c8faee0cb5df:38537/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://c8faee0cb5df:38537/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar at spark://c8faee0cb5df:38537/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar at spark://c8faee0cb5df:38537/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar at spark://c8faee0cb5df:38537/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://c8faee0cb5df:38537/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://c8faee0cb5df:38537/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://c8faee0cb5df:38537/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://c8faee0cb5df:38537/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://c8faee0cb5df:38537/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://c8faee0cb5df:38537/jars/com.typesafe_config-1.4.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://c8faee0cb5df:38537/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://c8faee0cb5df:38537/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://c8faee0cb5df:38537/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://c8faee0cb5df:38537/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://c8faee0cb5df:38537/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://c8faee0cb5df:38537/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar at spark://c8faee0cb5df:38537/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar at spark://c8faee0cb5df:38537/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar at spark://c8faee0cb5df:38537/files/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar at spark://c8faee0cb5df:38537/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at spark://c8faee0cb5df:38537/files/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.apache.kafka_kafka-clients-2.6.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://c8faee0cb5df:38537/files/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.apache.commons_commons-pool2-2.6.2.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://c8faee0cb5df:38537/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.spark-project.spark_unused-1.0.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at spark://c8faee0cb5df:38537/files/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.github.luben_zstd-jni-1.4.8-1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://c8faee0cb5df:38537/files/org.lz4_lz4-java-1.7.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.lz4_lz4-java-1.7.1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at spark://c8faee0cb5df:38537/files/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.xerial.snappy_snappy-java-1.1.8.2.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://c8faee0cb5df:38537/files/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.slf4j_slf4j-api-1.7.30.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar at spark://c8faee0cb5df:38537/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar at spark://c8faee0cb5df:38537/files/com.datastax.oss_java-driver-core-shaded-4.12.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.oss_java-driver-core-shaded-4.12.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar at spark://c8faee0cb5df:38537/files/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://c8faee0cb5df:38537/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.apache.commons_commons-lang3-3.10.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://c8faee0cb5df:38537/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.thoughtworks.paranamer_paranamer-2.8.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://c8faee0cb5df:38537/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.scala-lang_scala-reflect-2.12.11.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://c8faee0cb5df:38537/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.oss_native-protocol-1.5.0.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://c8faee0cb5df:38537/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://c8faee0cb5df:38537/files/com.typesafe_config-1.4.1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.typesafe_config-1.4.1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://c8faee0cb5df:38537/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/io.dropwizard.metrics_metrics-core-4.1.18.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://c8faee0cb5df:38537/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.hdrhistogram_HdrHistogram-2.1.12.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://c8faee0cb5df:38537/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/org.reactivestreams_reactive-streams-1.0.3.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://c8faee0cb5df:38537/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://c8faee0cb5df:38537/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://c8faee0cb5df:38537/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.google.code.findbugs_jsr305-3.0.2.jar
24/09/12 12:05:58 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar at spark://c8faee0cb5df:38537/files/com.datastax.oss_java-driver-query-builder-4.12.0.jar with timestamp 1726142758143
24/09/12 12:05:58 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar to /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3/com.datastax.oss_java-driver-query-builder-4.12.0.jar
24/09/12 12:05:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
24/09/12 12:05:58 INFO TransportClientFactory: Successfully created connection to spark-master/172.29.0.2:7077 after 13 ms (0 ms spent in bootstraps)
24/09/12 12:05:58 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240912120558-0002
24/09/12 12:05:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240912120558-0002/0 on worker-20240912115541-172.29.0.6-37399 (172.29.0.6:37399) with 2 core(s)
24/09/12 12:05:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20240912120558-0002/0 on hostPort 172.29.0.6:37399 with 2 core(s), 1024.0 MiB RAM
24/09/12 12:05:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43645.
24/09/12 12:05:58 INFO NettyBlockTransferService: Server created on c8faee0cb5df:43645
24/09/12 12:05:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/09/12 12:05:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:05:58 INFO BlockManagerMasterEndpoint: Registering block manager c8faee0cb5df:43645 with 434.4 MiB RAM, BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:05:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:05:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:05:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240912120558-0002/0 is now RUNNING
24/09/12 12:05:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-12 12:05:58,800 - DEBUG - Answer received: !yro20
2024-09-12 12:05:58,800 - DEBUG - Command to send: c
o20
sc
e

2024-09-12 12:05:58,802 - DEBUG - Answer received: !yro21
2024-09-12 12:05:58,802 - DEBUG - Command to send: c
o21
conf
e

2024-09-12 12:05:58,808 - DEBUG - Answer received: !yro22
2024-09-12 12:05:58,809 - DEBUG - Command to send: r
u
PythonAccumulatorV2
rj
e

2024-09-12 12:05:58,810 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
2024-09-12 12:05:58,811 - DEBUG - Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i53703
sc4e9aeb59a7b2ab0cea07f85f99307029b1ae69ea8001d75b4f6eca5e2209e68
e

2024-09-12 12:05:58,811 - DEBUG - Answer received: !yro23
2024-09-12 12:05:58,811 - DEBUG - Command to send: c
o20
sc
e

2024-09-12 12:05:58,811 - DEBUG - Answer received: !yro24
2024-09-12 12:05:58,811 - DEBUG - Command to send: c
o24
register
ro23
e

2024-09-12 12:05:58,812 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,813 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2024-09-12 12:05:58,814 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-09-12 12:05:58,814 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

2024-09-12 12:05:58,814 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,814 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro20
e

2024-09-12 12:05:58,815 - DEBUG - Answer received: !ybfalse
2024-09-12 12:05:58,815 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2024-09-12 12:05:58,816 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-09-12 12:05:58,816 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

2024-09-12 12:05:58,816 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,816 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro20
e

2024-09-12 12:05:58,819 - DEBUG - Answer received: !yL15
2024-09-12 12:05:58,819 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2024-09-12 12:05:58,820 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-09-12 12:05:58,820 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

2024-09-12 12:05:58,820 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,820 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro20
e

2024-09-12 12:05:58,823 - DEBUG - Answer received: !yi65536
2024-09-12 12:05:58,823 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,824 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,824 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,825 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,825 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,825 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,825 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,826 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,826 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,826 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,826 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,826 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,826 - DEBUG - Command to send: c
o22
get
sspark.submit.pyFiles
s
e

2024-09-12 12:05:58,827 - DEBUG - Answer received: !ys/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar,/opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar,/opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar,/opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar,/opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,/opt/bitnami/spark/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,/opt/bitnami/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,/opt/bitnami/spark/.ivy2/jars/com.typesafe_config-1.4.1.jar,/opt/bitnami/spark/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,/opt/bitnami/spark/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,/opt/bitnami/spark/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,/opt/bitnami/spark/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,/opt/bitnami/spark/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,/opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,/opt/bitnami/spark/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar
2024-09-12 12:05:58,827 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,827 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,827 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,828 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,828 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,828 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,828 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,829 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,829 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,829 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,829 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,829 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,829 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,830 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,830 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,830 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,830 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,831 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,831 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,831 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,831 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,831 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,831 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,831 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,831 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,832 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,832 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,832 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,832 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,832 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,832 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,832 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,832 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,833 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,833 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,833 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,833 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,834 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,834 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,834 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,834 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,834 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,834 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,835 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,835 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,835 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,835 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,835 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,835 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,835 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,835 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,835 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,836 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,836 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,836 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,836 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,836 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,836 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,836 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,836 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,837 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,837 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,837 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,838 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,838 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,838 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,838 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,838 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,838 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,839 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,839 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,839 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,839 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,842 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,842 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,842 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,842 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,843 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,843 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,843 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,843 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,843 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,843 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,844 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,844 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,846 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,846 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,847 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,847 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,847 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,847 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,848 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,848 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,848 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,849 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,849 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,849 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,850 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,850 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,851 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,851 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,851 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,851 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,851 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,851 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,851 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,851 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,851 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,851 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,852 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,852 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,852 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,852 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,855 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,855 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,855 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,855 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,856 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,856 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,856 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,856 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,857 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,857 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,858 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,858 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,858 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,858 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,858 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,858 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,858 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,859 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,859 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,859 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,859 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,860 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,860 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,860 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,860 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,860 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,860 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,860 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,860 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,861 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,861 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,861 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,862 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,862 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,862 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,862 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,862 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,862 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,862 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,862 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,863 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,863 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,863 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,863 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,864 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,864 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,864 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,864 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,864 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,864 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,864 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,864 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,864 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,864 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,864 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,865 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,865 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,865 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,866 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,866 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,868 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,868 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,868 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,868 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,868 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,868 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,868 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,868 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,869 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,869 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,869 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,869 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,870 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,870 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,870 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,870 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,870 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,870 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,870 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,870 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,870 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,870 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,870 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,870 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,871 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,871 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,871 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,871 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,871 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,871 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,871 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,871 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,872 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,872 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,873 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,873 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,873 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,873 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,873 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,873 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,873 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,873 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,873 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,873 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,874 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,874 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,874 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,874 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,874 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,874 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,874 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,874 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,874 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,874 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,874 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,874 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,875 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,876 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,876 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,876 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,876 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,876 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,877 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,877 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,877 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,877 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,877 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,877 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,878 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,878 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,878 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,878 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,878 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,878 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,878 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,879 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,879 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,879 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,879 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,879 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,879 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,879 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,880 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,880 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,880 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,880 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,880 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,880 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,881 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,881 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,881 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,881 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,882 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,882 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,882 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,882 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,883 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,883 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,883 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,883 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,883 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,883 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,883 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,883 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,884 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,884 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,884 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,885 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,885 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,885 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,885 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,885 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,886 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,886 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,886 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,886 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,887 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,887 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,887 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,887 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,887 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,888 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,888 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,888 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,888 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,888 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,888 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,888 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,889 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,889 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,889 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,889 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,889 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,889 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-09-12 12:05:58,889 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2024-09-12 12:05:58,889 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,889 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,889 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-09-12 12:05:58,889 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/userFiles-200fcfdf-7829-467e-b978-201afb29caf3
2024-09-12 12:05:58,889 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,890 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,890 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,890 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,890 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,890 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,890 - DEBUG - Command to send: r
u
org.apache.spark.util
rj
e

2024-09-12 12:05:58,890 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,890 - DEBUG - Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-09-12 12:05:58,891 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils
2024-09-12 12:05:58,892 - DEBUG - Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

2024-09-12 12:05:58,893 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,893 - DEBUG - Command to send: c
o20
sc
e

2024-09-12 12:05:58,893 - DEBUG - Answer received: !yro25
2024-09-12 12:05:58,893 - DEBUG - Command to send: c
o25
conf
e

2024-09-12 12:05:58,894 - DEBUG - Answer received: !yro26
2024-09-12 12:05:58,894 - DEBUG - Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro26
e

2024-09-12 12:05:58,895 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90
2024-09-12 12:05:58,895 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:05:58,897 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,897 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:05:58,898 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,898 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:05:58,898 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,898 - DEBUG - Command to send: r
u
org.apache.spark.util
rj
e

2024-09-12 12:05:58,899 - DEBUG - Answer received: !yp
2024-09-12 12:05:58,899 - DEBUG - Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-09-12 12:05:58,899 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils
2024-09-12 12:05:58,899 - DEBUG - Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

2024-09-12 12:05:58,899 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,899 - DEBUG - Command to send: c
z:org.apache.spark.util.Utils
createTempDir
s/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90
spyspark
e

2024-09-12 12:05:58,900 - DEBUG - Answer received: !yro27
2024-09-12 12:05:58,900 - DEBUG - Command to send: c
o27
getAbsolutePath
e

2024-09-12 12:05:58,900 - DEBUG - Answer received: !ys/tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/pyspark-4379998f-7d97-4cbc-ab7d-ba1212cc9b04
2024-09-12 12:05:58,900 - DEBUG - Command to send: c
o22
get
sspark.python.profile
sfalse
e

2024-09-12 12:05:58,901 - DEBUG - Answer received: !ysfalse
2024-09-12 12:05:58,901 - DEBUG - Command to send: c
o22
get
sspark.python.profile.memory
sfalse
e

2024-09-12 12:05:58,901 - DEBUG - Answer received: !ysfalse
2024-09-12 12:05:58,901 - DEBUG - Command to send: r
u
SparkSession
rj
e

2024-09-12 12:05:58,916 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2024-09-12 12:05:58,917 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

2024-09-12 12:05:58,935 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,935 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

2024-09-12 12:05:58,938 - DEBUG - Answer received: !yro28
2024-09-12 12:05:58,938 - DEBUG - Command to send: c
o28
isDefined
e

2024-09-12 12:05:58,939 - DEBUG - Answer received: !ybfalse
2024-09-12 12:05:58,939 - DEBUG - Command to send: r
u
SparkSession
rj
e

2024-09-12 12:05:58,941 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2024-09-12 12:05:58,941 - DEBUG - Command to send: c
o20
sc
e

2024-09-12 12:05:58,941 - DEBUG - Answer received: !yro29
2024-09-12 12:05:58,942 - DEBUG - Command to send: i
java.util.HashMap
e

2024-09-12 12:05:58,942 - DEBUG - Answer received: !yao30
2024-09-12 12:05:58,942 - DEBUG - Command to send: c
o30
put
sspark.app.name
sSparkDataStreaming
e

2024-09-12 12:05:58,942 - DEBUG - Answer received: !yn
2024-09-12 12:05:58,942 - DEBUG - Command to send: c
o30
put
sspark.jars.packages
scom.datastax.spark:spark-cassandra-connector_2.12:3.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1
e

2024-09-12 12:05:58,942 - DEBUG - Answer received: !yn
2024-09-12 12:05:58,942 - DEBUG - Command to send: c
o30
put
sspark.cassandra.connection.host
scassandra_db
e

2024-09-12 12:05:58,942 - DEBUG - Answer received: !yn
2024-09-12 12:05:58,942 - DEBUG - Command to send: i
org.apache.spark.sql.SparkSession
ro29
ro30
e

2024-09-12 12:05:58,971 - DEBUG - Answer received: !yro31
2024-09-12 12:05:58,972 - DEBUG - Command to send: r
u
SparkSession
rj
e

2024-09-12 12:05:58,972 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2024-09-12 12:05:58,972 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

2024-09-12 12:05:58,973 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,973 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro31
e

2024-09-12 12:05:58,974 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,974 - DEBUG - Command to send: r
u
SparkSession
rj
e

2024-09-12 12:05:58,974 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2024-09-12 12:05:58,974 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

2024-09-12 12:05:58,975 - DEBUG - Answer received: !ym
2024-09-12 12:05:58,975 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro31
e

2024-09-12 12:05:58,975 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,975 - DEBUG - Command to send: c
o20
setLogLevel
sDEBUG
e

2024-09-12 12:05:58,977 - DEBUG - Answer received: !yv
2024-09-12 12:05:58,977 - INFO - Spark connection created
2024-09-12 12:05:58,977 - DEBUG - Command to send: c
o31
readStream
e

24/09/12 12:05:58 DEBUG FileSystem: Looking for FS supporting file
24/09/12 12:05:58 DEBUG FileSystem: looking for configuration option fs.file.impl
24/09/12 12:05:58 DEBUG FileSystem: Looking in service filesystems for implementation class
24/09/12 12:05:58 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
24/09/12 12:05:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/09/12 12:05:58 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.jars.packages -> com.datastax.spark:spark-cassandra-connector_2.12:3.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1
24/09/12 12:05:58 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.cassandra.connection.host -> cassandra_db
24/09/12 12:05:58 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.app.name -> SparkDataStreaming
24/09/12 12:05:58 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
24/09/12 12:05:58 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol jar
24/09/12 12:05:58 DEBUG FileSystem: Looking for FS supporting jar
24/09/12 12:05:58 DEBUG FileSystem: looking for configuration option fs.jar.impl
24/09/12 12:05:58 DEBUG FileSystem: Looking in service filesystems for implementation class
24/09/12 12:05:58 DEBUG FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation
2024-09-12 12:05:59,096 - DEBUG - Command to send: A
c4e9aeb59a7b2ab0cea07f85f99307029b1ae69ea8001d75b4f6eca5e2209e68

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o1
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o2
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o3
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o4
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o5
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o6
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,097 - DEBUG - Command to send: m
d
o30
e

2024-09-12 12:05:59,097 - DEBUG - Answer received: !yv
2024-09-12 12:05:59,365 - DEBUG - Answer received: !yro32
2024-09-12 12:05:59,365 - DEBUG - Command to send: c
o32
format
skafka
e

2024-09-12 12:05:59,365 - DEBUG - Answer received: !yro33
2024-09-12 12:05:59,365 - DEBUG - Command to send: c
o33
option
skafka.bootstrap.servers
sbroker:29092
e

2024-09-12 12:05:59,365 - DEBUG - Answer received: !yro34
2024-09-12 12:05:59,366 - DEBUG - Command to send: c
o34
option
ssubscribe
suser_data
e

2024-09-12 12:05:59,366 - DEBUG - Answer received: !yro35
2024-09-12 12:05:59,366 - DEBUG - Command to send: c
o35
option
sstartingOffsets
searliest
e

2024-09-12 12:05:59,366 - DEBUG - Answer received: !yro36
2024-09-12 12:05:59,366 - DEBUG - Command to send: c
o36
load
e

24/09/12 12:05:59 DEBUG CatalystSqlParser: Parsing command: spark_grouping_id
24/09/12 12:05:59 DEBUG TransportServer: New connection accepted for remote address /172.29.0.6:40650.
24/09/12 12:05:59 DEBUG TransportServer: New connection accepted for remote address /172.29.0.6:40658.
2024-09-12 12:05:59,792 - DEBUG - Answer received: !yro37
2024-09-12 12:05:59,792 - INFO - Successfully connected to Kafka
2024-09-12 12:05:59,792 - INFO - Connected to Kafka
2024-09-12 12:05:59,792 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2024-09-12 12:05:59,793 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-09-12 12:05:59,793 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-09-12 12:05:59,793 - DEBUG - Answer received: !ym
2024-09-12 12:05:59,793 - DEBUG - Command to send: i
java.util.ArrayList
e

2024-09-12 12:05:59,794 - DEBUG - Answer received: !ylo38
2024-09-12 12:05:59,794 - DEBUG - Command to send: c
o38
add
sCAST(value AS STRING) as json
e

2024-09-12 12:05:59,795 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:59,795 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro38
e

2024-09-12 12:05:59,795 - DEBUG - Answer received: !yro39
2024-09-12 12:05:59,795 - DEBUG - Command to send: c
o37
selectExpr
ro39
e

24/09/12 12:05:59 DEBUG SparkSqlParser: Parsing command: CAST(value AS STRING) as json
24/09/12 12:05:59 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#8
2024-09-12 12:05:59,844 - DEBUG - Answer received: !yro40
2024-09-12 12:05:59,844 - DEBUG - Command to send: r
u
functions
rj
e

2024-09-12 12:05:59,847 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2024-09-12 12:05:59,847 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-09-12 12:05:59,851 - DEBUG - Answer received: !ym
2024-09-12 12:05:59,851 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
col
sjson
e

2024-09-12 12:05:59,861 - DEBUG - Answer received: !yro41
2024-09-12 12:05:59,861 - DEBUG - Command to send: r
u
functions
rj
e

2024-09-12 12:05:59,861 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2024-09-12 12:05:59,862 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
from_json
e

2024-09-12 12:05:59,862 - DEBUG - Answer received: !ym
2024-09-12 12:05:59,862 - DEBUG - Command to send: i
java.util.HashMap
e

2024-09-12 12:05:59,862 - DEBUG - Answer received: !yao42
2024-09-12 12:05:59,862 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
from_json
ro41
s{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"first_name","nullable":true,"type":"string"},{"metadata":{},"name":"last_name","nullable":true,"type":"string"},{"metadata":{},"name":"gender","nullable":true,"type":"string"},{"metadata":{},"name":"address","nullable":true,"type":"string"},{"metadata":{},"name":"post_code","nullable":true,"type":"string"},{"metadata":{},"name":"email","nullable":true,"type":"string"},{"metadata":{},"name":"username","nullable":true,"type":"string"},{"metadata":{},"name":"dob","nullable":true,"type":"string"},{"metadata":{},"name":"registered_date","nullable":true,"type":"string"},{"metadata":{},"name":"phone","nullable":true,"type":"string"},{"metadata":{},"name":"picture","nullable":true,"type":"string"}],"type":"struct"}
ro42
e

24/09/12 12:05:59 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.29.0.6:40658) with ID 0,  ResourceProfileId 0
2024-09-12 12:05:59,882 - DEBUG - Answer received: !yro43
2024-09-12 12:05:59,882 - DEBUG - Command to send: c
o43
as
sdata
e

2024-09-12 12:05:59,883 - DEBUG - Answer received: !yro44
2024-09-12 12:05:59,883 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2024-09-12 12:05:59,884 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-09-12 12:05:59,884 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-09-12 12:05:59,884 - DEBUG - Answer received: !ym
2024-09-12 12:05:59,884 - DEBUG - Command to send: i
java.util.ArrayList
e

2024-09-12 12:05:59,884 - DEBUG - Answer received: !ylo45
2024-09-12 12:05:59,884 - DEBUG - Command to send: c
o45
add
ro44
e

2024-09-12 12:05:59,884 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:59,884 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro45
e

2024-09-12 12:05:59,885 - DEBUG - Answer received: !yro46
2024-09-12 12:05:59,885 - DEBUG - Command to send: c
o40
select
ro46
e

24/09/12 12:05:59 DEBUG Analyzer$ResolveReferences: Resolving 'json to json#21
24/09/12 12:05:59 DEBUG DefaultTopologyMapper: Got a request for 172.29.0.6
24/09/12 12:05:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.29.0.6:40323 with 434.4 MiB RAM, BlockManagerId(0, 172.29.0.6, 40323, None)
2024-09-12 12:05:59,920 - DEBUG - Answer received: !yro47
2024-09-12 12:05:59,920 - DEBUG - Command to send: r
u
functions
rj
e

2024-09-12 12:05:59,921 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2024-09-12 12:05:59,921 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-09-12 12:05:59,921 - DEBUG - Answer received: !ym
2024-09-12 12:05:59,921 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
col
sdata.*
e

2024-09-12 12:05:59,922 - DEBUG - Answer received: !yro48
2024-09-12 12:05:59,922 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2024-09-12 12:05:59,922 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-09-12 12:05:59,922 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-09-12 12:05:59,923 - DEBUG - Answer received: !ym
2024-09-12 12:05:59,923 - DEBUG - Command to send: i
java.util.ArrayList
e

2024-09-12 12:05:59,923 - DEBUG - Answer received: !ylo49
2024-09-12 12:05:59,923 - DEBUG - Command to send: c
o49
add
ro48
e

2024-09-12 12:05:59,923 - DEBUG - Answer received: !ybtrue
2024-09-12 12:05:59,923 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro49
e

2024-09-12 12:05:59,923 - DEBUG - Answer received: !yro50
2024-09-12 12:05:59,923 - DEBUG - Command to send: c
o47
select
ro50
e

24/09/12 12:05:59 DEBUG TransportServer: New connection accepted for remote address /172.29.0.6:40662.
2024-09-12 12:05:59,939 - DEBUG - Answer received: !yro51
2024-09-12 12:05:59,939 - INFO - Created DataFrame from Kafka stream
2024-09-12 12:05:59,981 - WARNING - Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra_db'], lbp = None)
2024-09-12 12:05:59,981 - DEBUG - Connecting to cluster, contact points: ['cassandra_db']; protocol version: 66
2024-09-12 12:05:59,981 - DEBUG - Host 172.29.0.3:9042 is now marked up
2024-09-12 12:05:59,981 - DEBUG - [control connection] Opening new connection to 172.29.0.3:9042
2024-09-12 12:05:59,982 - DEBUG - Sending initial options message for new connection (281472827670352) to 172.29.0.3:9042
2024-09-12 12:05:59,982 - DEBUG - Starting libev event loop
2024-09-12 12:05:59,983 - DEBUG - Defuncting connection (281472827670352) to 172.29.0.3:9042: <Error from server: code=000a [Protocol error] message="Invalid or unsupported protocol version (66); supported versions are (3/v3, 4/v4, 5/v5, 6/v6-beta)">
2024-09-12 12:05:59,983 - DEBUG - Closing connection (281472827670352) to 172.29.0.3:9042
2024-09-12 12:05:59,983 - DEBUG - Closed socket to 172.29.0.3:9042
2024-09-12 12:05:59,983 - DEBUG - All Connections currently closed, event loop ended
2024-09-12 12:05:59,983 - WARNING - Downgrading core protocol version from 66 to 65 for 172.29.0.3:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
2024-09-12 12:05:59,983 - DEBUG - Sending initial options message for new connection (281472827670352) to 172.29.0.3:9042
2024-09-12 12:05:59,983 - DEBUG - Starting libev event loop
2024-09-12 12:05:59,985 - DEBUG - Defuncting connection (281472827670352) to 172.29.0.3:9042: <Error from server: code=000a [Protocol error] message="Invalid or unsupported protocol version (65); supported versions are (3/v3, 4/v4, 5/v5, 6/v6-beta)">
2024-09-12 12:05:59,985 - DEBUG - Closing connection (281472827670352) to 172.29.0.3:9042
2024-09-12 12:05:59,985 - DEBUG - Closed socket to 172.29.0.3:9042
2024-09-12 12:05:59,986 - DEBUG - All Connections currently closed, event loop ended
2024-09-12 12:05:59,986 - WARNING - Downgrading core protocol version from 65 to 5 for 172.29.0.3:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
2024-09-12 12:05:59,986 - DEBUG - Sending initial options message for new connection (281472827670352) to 172.29.0.3:9042
2024-09-12 12:05:59,986 - DEBUG - Starting libev event loop
2024-09-12 12:05:59,987 - DEBUG - Received options response on new connection (281472827670352) from 172.29.0.3:9042
2024-09-12 12:05:59,987 - DEBUG - No available compression types supported on both ends. locally supported: odict_keys([]). remotely supported: ['snappy', 'lz4']
2024-09-12 12:05:59,987 - DEBUG - Sending StartupMessage on <LibevConnection(281472827670352) 172.29.0.3:9042>
2024-09-12 12:05:59,987 - DEBUG - Sent StartupMessage on <LibevConnection(281472827670352) 172.29.0.3:9042>
2024-09-12 12:05:59,988 - DEBUG - Got ReadyMessage on new connection (281472827670352) from 172.29.0.3:9042
2024-09-12 12:05:59,988 - DEBUG - Enabling protocol checksumming on connection (281472827670352).
2024-09-12 12:05:59,988 - DEBUG - [control connection] Established new connection <LibevConnection(281472827670352) 172.29.0.3:9042>, registering watchers and refreshing schema and topology
2024-09-12 12:05:59,995 - DEBUG - [control connection] Refreshing node list and token map using preloaded results
2024-09-12 12:05:59,995 - INFO - Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '172.29.0.3:9042'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
2024-09-12 12:05:59,995 - DEBUG - [control connection] Finished fetching ring info
2024-09-12 12:05:59,995 - DEBUG - [control connection] Rebuilding token map due to topology changes
2024-09-12 12:06:00,010 - DEBUG - Control connection created
2024-09-12 12:06:00,010 - DEBUG - Initializing connection for host 172.29.0.3:9042
2024-09-12 12:06:00,011 - DEBUG - Sending initial options message for new connection (281472799621728) to 172.29.0.3:9042
2024-09-12 12:06:00,011 - DEBUG - Received options response on new connection (281472799621728) from 172.29.0.3:9042
2024-09-12 12:06:00,011 - DEBUG - No available compression types supported on both ends. locally supported: odict_keys([]). remotely supported: ['snappy', 'lz4']
2024-09-12 12:06:00,012 - DEBUG - Sending StartupMessage on <LibevConnection(281472799621728) 172.29.0.3:9042>
2024-09-12 12:06:00,012 - DEBUG - Sent StartupMessage on <LibevConnection(281472799621728) 172.29.0.3:9042>
2024-09-12 12:06:00,012 - DEBUG - Got ReadyMessage on new connection (281472799621728) from 172.29.0.3:9042
2024-09-12 12:06:00,012 - DEBUG - Enabling protocol checksumming on connection (281472799621728).
2024-09-12 12:06:00,012 - DEBUG - Finished initializing connection for host 172.29.0.3:9042
2024-09-12 12:06:00,012 - DEBUG - Added pool for host 172.29.0.3:9042 to session
2024-09-12 12:06:00,013 - DEBUG - Not starting MonitorReporter thread for Insights; not supported by server version 5.0.0 on ControlConnection host 172.29.0.3:9042
2024-09-12 12:06:00,013 - DEBUG - Started Session with client_id 260bc965-436d-442c-bd91-9eb299162dcb and session_id db7bae6e-c9f3-433a-9857-89e6ac89c6d4
2024-09-12 12:06:00,013 - INFO - Cassandra connection created
2024-09-12 12:06:00,014 - INFO - Keyspace created
2024-09-12 12:06:00,015 - INFO - Table created successfully!
2024-09-12 12:06:00,016 - DEBUG - Command to send: c
o51
writeStream
e

2024-09-12 12:06:00,025 - DEBUG - Answer received: !yro52
2024-09-12 12:06:00,025 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.execution.streaming.sources.*
e

2024-09-12 12:06:00,025 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,025 - DEBUG - Command to send: r
u
PythonForeachBatchHelper
rj
e

2024-09-12 12:06:00,026 - DEBUG - Answer received: !ycorg.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper
2024-09-12 12:06:00,026 - DEBUG - Command to send: r
m
org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper
callForeachBatch
e

2024-09-12 12:06:00,027 - DEBUG - Answer received: !ym
2024-09-12 12:06:00,027 - DEBUG - Command to send: c
z:org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper
callForeachBatch
ro52
fp0;org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchFunction
e

2024-09-12 12:06:00,030 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,031 - INFO - Callback Server Starting
2024-09-12 12:06:00,031 - INFO - Socket listening on ('127.0.0.1', 43491)
2024-09-12 12:06:00,031 - DEBUG - Command to send: c
GATEWAY_SERVER
getCallbackClient
e

2024-09-12 12:06:00,032 - DEBUG - Answer received: !yro53
2024-09-12 12:06:00,032 - DEBUG - Command to send: c
o53
getAddress
e

2024-09-12 12:06:00,033 - DEBUG - Answer received: !yro54
2024-09-12 12:06:00,033 - DEBUG - Command to send: c
GATEWAY_SERVER
resetCallbackClient
ro54
i43491
e

2024-09-12 12:06:00,033 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,034 - DEBUG - Command to send: c
o52
option
scheckpointLocation
s/tmp/checkpoint
e

2024-09-12 12:06:00,036 - DEBUG - Answer received: !yro55
2024-09-12 12:06:00,036 - DEBUG - Command to send: c
o55
start
e

24/09/12 12:06:00 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/09/12 12:06:00 DEBUG StreamingQueryStatisticsPage: Supported custom metrics: List(StateStoreCustomSizeMetric(stateOnCurrentVersionSizeBytes,estimated size of state only on current version), StateStoreCustomSumMetric(loadedMapCacheHitCount,count of cache hit on states cache in provider), StateStoreCustomSumMetric(loadedMapCacheMissCount,count of cache miss on states cache in provider))
24/09/12 12:06:00 DEBUG StreamingQueryStatisticsPage: Enabled custom metrics: ArrayBuffer()
24/09/12 12:06:00 DEBUG UserGroupInformation: PrivilegedAction [as: spark (auth:SIMPLE)][action: org.apache.hadoop.fs.FileContext$2@1dca8c4]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:465)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:491)
	at org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:309)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:352)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$.create(CheckpointFileManager.scala:209)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:98)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:270)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:346)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:433)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:368)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
24/09/12 12:06:00 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoint resolved to file:/tmp/checkpoint.
24/09/12 12:06:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2024-09-12 12:06:00,098 - DEBUG - Command to send: m
d
o38
e

2024-09-12 12:06:00,098 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,098 - DEBUG - Command to send: m
d
o42
e

2024-09-12 12:06:00,098 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,098 - DEBUG - Command to send: m
d
o45
e

2024-09-12 12:06:00,098 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,098 - DEBUG - Command to send: m
d
o0
e

2024-09-12 12:06:00,098 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o7
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o8
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o9
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o10
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o11
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o12
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,099 - DEBUG - Command to send: m
d
o13
e

2024-09-12 12:06:00,099 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,100 - DEBUG - Command to send: m
d
o14
e

2024-09-12 12:06:00,100 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,100 - DEBUG - Command to send: m
d
o15
e

2024-09-12 12:06:00,100 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,100 - DEBUG - Command to send: m
d
o16
e

2024-09-12 12:06:00,100 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,100 - DEBUG - Command to send: m
d
o17
e

2024-09-12 12:06:00,100 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,100 - DEBUG - Command to send: m
d
o18
e

2024-09-12 12:06:00,101 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,101 - DEBUG - Command to send: m
d
o19
e

2024-09-12 12:06:00,101 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,101 - DEBUG - Command to send: m
d
o21
e

2024-09-12 12:06:00,101 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,101 - DEBUG - Command to send: m
d
o24
e

2024-09-12 12:06:00,101 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,101 - DEBUG - Command to send: m
d
o25
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o26
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o27
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o28
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o29
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o32
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o33
e

2024-09-12 12:06:00,102 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,102 - DEBUG - Command to send: m
d
o34
e

2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o35
e

2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o36
e

2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o39
e

2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o40
e

2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o41
e

24/09/12 12:06:00 DEBUG UserGroupInformation: PrivilegedAction [as: spark (auth:SIMPLE)][action: org.apache.hadoop.fs.FileContext$2@1c596b09]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:465)
	at org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:311)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:352)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$.create(CheckpointFileManager.scala:209)
	at org.apache.spark.sql.execution.streaming.StreamMetadata$.read(StreamMetadata.scala:52)
	at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:137)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:295)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:346)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:433)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:368)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o43
e

2024-09-12 12:06:00,103 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,103 - DEBUG - Command to send: m
d
o44
e

2024-09-12 12:06:00,104 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,104 - DEBUG - Command to send: m
d
o46
e

2024-09-12 12:06:00,104 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,104 - DEBUG - Command to send: m
d
o49
e

2024-09-12 12:06:00,104 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,104 - DEBUG - Command to send: m
d
o50
e

2024-09-12 12:06:00,104 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,104 - DEBUG - Command to send: m
d
o48
e

2024-09-12 12:06:00,105 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,105 - DEBUG - Command to send: m
d
o47
e

2024-09-12 12:06:00,105 - DEBUG - Answer received: !yv
24/09/12 12:06:00 DEBUG UserGroupInformation: PrivilegedAction [as: spark (auth:SIMPLE)][action: org.apache.hadoop.fs.FileContext$2@3f32a408]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:465)
	at org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:311)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:352)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$.create(CheckpointFileManager.scala:209)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:64)
	at org.apache.spark.sql.execution.streaming.OffsetSeqLog.<init>(OffsetSeqLog.scala:47)
	at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:222)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:295)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:346)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:433)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:368)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
24/09/12 12:06:00 DEBUG UserGroupInformation: PrivilegedAction [as: spark (auth:SIMPLE)][action: org.apache.hadoop.fs.FileContext$2@50e0e78]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:465)
	at org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:311)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:352)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$.create(CheckpointFileManager.scala:209)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:64)
	at org.apache.spark.sql.execution.streaming.CommitLog.<init>(CommitLog.scala:49)
	at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:229)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:295)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:346)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:433)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:368)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
24/09/12 12:06:00 DEBUG UserGroupInformation: PrivilegedAction [as: spark (auth:SIMPLE)][action: org.apache.hadoop.fs.FileContext$2@76cd791e]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:465)
	at org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:311)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.<init>(CheckpointFileManager.scala:352)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$.create(CheckpointFileManager.scala:209)
	at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:246)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:295)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:346)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:433)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:368)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
24/09/12 12:06:00 INFO MicroBatchExecution: Starting [id = 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, runId = 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d]. Use file:/tmp/checkpoint to store the query checkpoint.
2024-09-12 12:06:00,159 - DEBUG - Answer received: !yro56
2024-09-12 12:06:00,159 - INFO - Streaming query started
2024-09-12 12:06:00,159 - DEBUG - Command to send: c
o56
awaitTermination
e

24/09/12 12:06:00 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@16e675d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4b381dd]
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: source: Set key.deserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: source: Set value.deserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: source: Set auto.offset.reset to earliest, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: source: Set enable.auto.commit to false, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: source: Set max.poll.records to 1, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: source: Set receive.buffer.bytes to 65536
24/09/12 12:06:00 DEBUG KafkaOffsetReader: Creating new Admin based offset reader
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: executor: Set key.deserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: executor: Set value.deserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: executor: Set auto.offset.reset to none, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: executor: Set group.id to spark-kafka-source-3c887d1e-d125-4faa-bd30-632b7a8d7b0b--2090736517-executor
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: executor: Set enable.auto.commit to false, earlier value: 
24/09/12 12:06:00 DEBUG KafkaConfigUpdater: executor: Set receive.buffer.bytes to 65536
24/09/12 12:06:00 DEBUG MicroBatchExecution: Starting Trigger Calculation
24/09/12 12:06:00 INFO OffsetSeqLog: BatchIds found from listing: 0, 1
24/09/12 12:06:00 INFO OffsetSeqLog: Getting latest batch 1
24/09/12 12:06:00 INFO OffsetSeqLog: BatchIds found from listing: 0, 1
24/09/12 12:06:00 INFO OffsetSeqLog: Getting latest batch 1
24/09/12 12:06:00 INFO CommitLog: BatchIds found from listing: 0
24/09/12 12:06:00 INFO CommitLog: Getting latest batch 0
24/09/12 12:06:00 INFO MicroBatchExecution: Resuming at batch 1 with committed offsets {KafkaV2[Subscribe[user_data]]: {"user_data":{"0":0}}} and available offsets {KafkaV2[Subscribe[user_data]]: {"user_data":{"0":1}}}
24/09/12 12:06:00 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[user_data]]: {"user_data":{"0":0}}}
24/09/12 12:06:00 DEBUG MicroBatchExecution: Running batch 1
24/09/12 12:06:00 DEBUG MicroBatchExecution: Retrieving data from KafkaV2[Subscribe[user_data]]: Some({"user_data":{"0":0}}) -> {"user_data":{"0":1}}
24/09/12 12:06:00 DEBUG MicroBatchExecution: getBatch took 8 ms
24/09/12 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/12 12:06:00 DEBUG KafkaOffsetReaderAdmin: TopicPartitions: user_data-0
24/09/12 12:06:00 DEBUG PropagateWatermarkSimulator: watermark for batch ID 0 is received as 0, call site: org.apache.spark.sql.execution.streaming.PropagateWatermarkSimulator.$anonfun$propagate$3(WatermarkPropagator.scala:252)
org.apache.spark.internal.Logging.logDebug(Logging.scala:64)
org.apache.spark.internal.Logging.logDebug$(Logging.scala:63)
org.apache.spark.sql.execution.streaming.PropagateWatermarkSimulator.logDebug(WatermarkPropagator.scala:155)
org.apache.spark.sql.execution.streaming.PropagateWatermarkSimulator.propagate(WatermarkPropagator.scala:251)
org.apache.spark.sql.execution.streaming.IncrementalExecution$$anon$2.simulateWatermarkPropagation(IncrementalExecution.scala:345)
org.apache.spark.sql.execution.streaming.IncrementalExecution$$anon$2.apply(IncrementalExecution.scala:360)
org.apache.spark.sql.execution.streaming.IncrementalExecution$$anon$2.apply(IncrementalExecution.scala:339)
org.apache.spark.sql.execution.QueryExecution$.$anonfun$prepareForExecution$1(QueryExecution.scala:477)
scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
scala.collection.immutable.List.foldLeft(List.scala:91)
org.apache.spark.sql.execution.QueryExecution$.prepareForExecution(QueryExecution.scala:476)
org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:186)
org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
24/09/12 12:06:00 DEBUG PropagateWatermarkSimulator: skipping the propagation for batch 0 as origin watermark is 0.
24/09/12 12:06:00 DEBUG PropagateWatermarkSimulator: watermark for batch ID 1 is received as 0, call site: org.apache.spark.sql.execution.streaming.PropagateWatermarkSimulator.$anonfun$propagate$3(WatermarkPropagator.scala:252)
org.apache.spark.internal.Logging.logDebug(Logging.scala:64)
org.apache.spark.internal.Logging.logDebug$(Logging.scala:63)
org.apache.spark.sql.execution.streaming.PropagateWatermarkSimulator.logDebug(WatermarkPropagator.scala:155)
org.apache.spark.sql.execution.streaming.PropagateWatermarkSimulator.propagate(WatermarkPropagator.scala:251)
org.apache.spark.sql.execution.streaming.IncrementalExecution$$anon$2.simulateWatermarkPropagation(IncrementalExecution.scala:347)
org.apache.spark.sql.execution.streaming.IncrementalExecution$$anon$2.apply(IncrementalExecution.scala:360)
org.apache.spark.sql.execution.streaming.IncrementalExecution$$anon$2.apply(IncrementalExecution.scala:339)
org.apache.spark.sql.execution.QueryExecution$.$anonfun$prepareForExecution$1(QueryExecution.scala:477)
scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
scala.collection.immutable.List.foldLeft(List.scala:91)
org.apache.spark.sql.execution.QueryExecution$.prepareForExecution(QueryExecution.scala:476)
org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:186)
org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
24/09/12 12:06:00 DEBUG PropagateWatermarkSimulator: skipping the propagation for batch 1 as origin watermark is 0.
24/09/12 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/12 12:06:00 DEBUG KafkaOffsetReaderAdmin: TopicPartitions: user_data-0
24/09/12 12:06:00 DEBUG MicroBatchExecution: queryPlanning took 221 ms
24/09/12 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/12 12:06:00 DEBUG KafkaOffsetReaderAdmin: TopicPartitions: user_data-0
24/09/12 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/12 12:06:00 DEBUG KafkaOffsetReaderAdmin: TopicPartitions: user_data-0
24/09/12 12:06:00 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   private void project_doConsume_0(InternalRow inputadapter_row_0, InternalRow project_expr_0_0, boolean project_exprIsNull_0_0) throws java.io.IOException {
/* 025 */     // common sub-expressions
/* 026 */
/* 027 */     boolean project_isNull_0 = project_exprIsNull_0_0;
/* 028 */     UTF8String project_value_0 = null;
/* 029 */
/* 030 */     if (!project_exprIsNull_0_0) {
/* 031 */       if (project_expr_0_0.isNullAt(0)) {
/* 032 */         project_isNull_0 = true;
/* 033 */       } else {
/* 034 */         project_value_0 = project_expr_0_0.getUTF8String(0);
/* 035 */       }
/* 036 */
/* 037 */     }
/* 038 */     boolean project_isNull_2 = project_exprIsNull_0_0;
/* 039 */     UTF8String project_value_2 = null;
/* 040 */
/* 041 */     if (!project_exprIsNull_0_0) {
/* 042 */       if (project_expr_0_0.isNullAt(1)) {
/* 043 */         project_isNull_2 = true;
/* 044 */       } else {
/* 045 */         project_value_2 = project_expr_0_0.getUTF8String(1);
/* 046 */       }
/* 047 */
/* 048 */     }
/* 049 */     boolean project_isNull_4 = project_exprIsNull_0_0;
/* 050 */     UTF8String project_value_4 = null;
/* 051 */
/* 052 */     if (!project_exprIsNull_0_0) {
/* 053 */       if (project_expr_0_0.isNullAt(2)) {
/* 054 */         project_isNull_4 = true;
/* 055 */       } else {
/* 056 */         project_value_4 = project_expr_0_0.getUTF8String(2);
/* 057 */       }
/* 058 */
/* 059 */     }
/* 060 */     boolean project_isNull_6 = project_exprIsNull_0_0;
/* 061 */     UTF8String project_value_6 = null;
/* 062 */
/* 063 */     if (!project_exprIsNull_0_0) {
/* 064 */       if (project_expr_0_0.isNullAt(3)) {
/* 065 */         project_isNull_6 = true;
/* 066 */       } else {
/* 067 */         project_value_6 = project_expr_0_0.getUTF8String(3);
/* 068 */       }
/* 069 */
/* 070 */     }
/* 071 */     boolean project_isNull_8 = project_exprIsNull_0_0;
/* 072 */     UTF8String project_value_8 = null;
/* 073 */
/* 074 */     if (!project_exprIsNull_0_0) {
/* 075 */       if (project_expr_0_0.isNullAt(4)) {
/* 076 */         project_isNull_8 = true;
/* 077 */       } else {
/* 078 */         project_value_8 = project_expr_0_0.getUTF8String(4);
/* 079 */       }
/* 080 */
/* 081 */     }
/* 082 */     boolean project_isNull_10 = project_exprIsNull_0_0;
/* 083 */     UTF8String project_value_10 = null;
/* 084 */
/* 085 */     if (!project_exprIsNull_0_0) {
/* 086 */       if (project_expr_0_0.isNullAt(5)) {
/* 087 */         project_isNull_10 = true;
/* 088 */       } else {
/* 089 */         project_value_10 = project_expr_0_0.getUTF8String(5);
/* 090 */       }
/* 091 */
/* 092 */     }
/* 093 */     boolean project_isNull_12 = project_exprIsNull_0_0;
/* 094 */     UTF8String project_value_12 = null;
/* 095 */
/* 096 */     if (!project_exprIsNull_0_0) {
/* 097 */       if (project_expr_0_0.isNullAt(6)) {
/* 098 */         project_isNull_12 = true;
/* 099 */       } else {
/* 100 */         project_value_12 = project_expr_0_0.getUTF8String(6);
/* 101 */       }
/* 102 */
/* 103 */     }
/* 104 */     boolean project_isNull_14 = project_exprIsNull_0_0;
/* 105 */     UTF8String project_value_14 = null;
/* 106 */
/* 107 */     if (!project_exprIsNull_0_0) {
/* 108 */       if (project_expr_0_0.isNullAt(7)) {
/* 109 */         project_isNull_14 = true;
/* 110 */       } else {
/* 111 */         project_value_14 = project_expr_0_0.getUTF8String(7);
/* 112 */       }
/* 113 */
/* 114 */     }
/* 115 */     boolean project_isNull_16 = project_exprIsNull_0_0;
/* 116 */     UTF8String project_value_16 = null;
/* 117 */
/* 118 */     if (!project_exprIsNull_0_0) {
/* 119 */       if (project_expr_0_0.isNullAt(8)) {
/* 120 */         project_isNull_16 = true;
/* 121 */       } else {
/* 122 */         project_value_16 = project_expr_0_0.getUTF8String(8);
/* 123 */       }
/* 124 */
/* 125 */     }
/* 126 */     boolean project_isNull_18 = project_exprIsNull_0_0;
/* 127 */     UTF8String project_value_18 = null;
/* 128 */
/* 129 */     if (!project_exprIsNull_0_0) {
/* 130 */       if (project_expr_0_0.isNullAt(9)) {
/* 131 */         project_isNull_18 = true;
/* 132 */       } else {
/* 133 */         project_value_18 = project_expr_0_0.getUTF8String(9);
/* 134 */       }
/* 135 */
/* 136 */     }
/* 137 */     boolean project_isNull_20 = project_exprIsNull_0_0;
/* 138 */     UTF8String project_value_20 = null;
/* 139 */
/* 140 */     if (!project_exprIsNull_0_0) {
/* 141 */       if (project_expr_0_0.isNullAt(10)) {
/* 142 */         project_isNull_20 = true;
/* 143 */       } else {
/* 144 */         project_value_20 = project_expr_0_0.getUTF8String(10);
/* 145 */       }
/* 146 */
/* 147 */     }
/* 148 */     boolean project_isNull_22 = project_exprIsNull_0_0;
/* 149 */     UTF8String project_value_22 = null;
/* 150 */
/* 151 */     if (!project_exprIsNull_0_0) {
/* 152 */       if (project_expr_0_0.isNullAt(11)) {
/* 153 */         project_isNull_22 = true;
/* 154 */       } else {
/* 155 */         project_value_22 = project_expr_0_0.getUTF8String(11);
/* 156 */       }
/* 157 */
/* 158 */     }
/* 159 */     project_mutableStateArray_0[0].reset();
/* 160 */
/* 161 */     project_mutableStateArray_0[0].zeroOutNullBytes();
/* 162 */
/* 163 */     if (project_isNull_0) {
/* 164 */       project_mutableStateArray_0[0].setNullAt(0);
/* 165 */     } else {
/* 166 */       project_mutableStateArray_0[0].write(0, project_value_0);
/* 167 */     }
/* 168 */
/* 169 */     if (project_isNull_2) {
/* 170 */       project_mutableStateArray_0[0].setNullAt(1);
/* 171 */     } else {
/* 172 */       project_mutableStateArray_0[0].write(1, project_value_2);
/* 173 */     }
/* 174 */
/* 175 */     if (project_isNull_4) {
/* 176 */       project_mutableStateArray_0[0].setNullAt(2);
/* 177 */     } else {
/* 178 */       project_mutableStateArray_0[0].write(2, project_value_4);
/* 179 */     }
/* 180 */
/* 181 */     if (project_isNull_6) {
/* 182 */       project_mutableStateArray_0[0].setNullAt(3);
/* 183 */     } else {
/* 184 */       project_mutableStateArray_0[0].write(3, project_value_6);
/* 185 */     }
/* 186 */
/* 187 */     if (project_isNull_8) {
/* 188 */       project_mutableStateArray_0[0].setNullAt(4);
/* 189 */     } else {
/* 190 */       project_mutableStateArray_0[0].write(4, project_value_8);
/* 191 */     }
/* 192 */
/* 193 */     if (project_isNull_10) {
/* 194 */       project_mutableStateArray_0[0].setNullAt(5);
/* 195 */     } else {
/* 196 */       project_mutableStateArray_0[0].write(5, project_value_10);
/* 197 */     }
/* 198 */
/* 199 */     if (project_isNull_12) {
/* 200 */       project_mutableStateArray_0[0].setNullAt(6);
/* 201 */     } else {
/* 202 */       project_mutableStateArray_0[0].write(6, project_value_12);
/* 203 */     }
/* 204 */
/* 205 */     if (project_isNull_14) {
/* 206 */       project_mutableStateArray_0[0].setNullAt(7);
/* 207 */     } else {
/* 208 */       project_mutableStateArray_0[0].write(7, project_value_14);
/* 209 */     }
/* 210 */
/* 211 */     if (project_isNull_16) {
/* 212 */       project_mutableStateArray_0[0].setNullAt(8);
/* 213 */     } else {
/* 214 */       project_mutableStateArray_0[0].write(8, project_value_16);
/* 215 */     }
/* 216 */
/* 217 */     if (project_isNull_18) {
/* 218 */       project_mutableStateArray_0[0].setNullAt(9);
/* 219 */     } else {
/* 220 */       project_mutableStateArray_0[0].write(9, project_value_18);
/* 221 */     }
/* 222 */
/* 223 */     if (project_isNull_20) {
/* 224 */       project_mutableStateArray_0[0].setNullAt(10);
/* 225 */     } else {
/* 226 */       project_mutableStateArray_0[0].write(10, project_value_20);
/* 227 */     }
/* 228 */
/* 229 */     if (project_isNull_22) {
/* 230 */       project_mutableStateArray_0[0].setNullAt(11);
/* 231 */     } else {
/* 232 */       project_mutableStateArray_0[0].write(11, project_value_22);
/* 233 */     }
/* 234 */     append((project_mutableStateArray_0[0].getRow()));
/* 235 */
/* 236 */   }
/* 237 */
/* 238 */   protected void processNext() throws java.io.IOException {
/* 239 */     while ( inputadapter_input_0.hasNext()) {
/* 240 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 241 */
/* 242 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 243 */       InternalRow inputadapter_value_0 = inputadapter_isNull_0 ?
/* 244 */       null : (inputadapter_row_0.getStruct(0, 12));
/* 245 */
/* 246 */       project_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);
/* 247 */       if (shouldStop()) return;
/* 248 */     }
/* 249 */   }
/* 250 */
/* 251 */ }

24/09/12 12:06:00 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   private void project_doConsume_0(InternalRow inputadapter_row_0, InternalRow project_expr_0_0, boolean project_exprIsNull_0_0) throws java.io.IOException {
/* 025 */     // common sub-expressions
/* 026 */
/* 027 */     boolean project_isNull_0 = project_exprIsNull_0_0;
/* 028 */     UTF8String project_value_0 = null;
/* 029 */
/* 030 */     if (!project_exprIsNull_0_0) {
/* 031 */       if (project_expr_0_0.isNullAt(0)) {
/* 032 */         project_isNull_0 = true;
/* 033 */       } else {
/* 034 */         project_value_0 = project_expr_0_0.getUTF8String(0);
/* 035 */       }
/* 036 */
/* 037 */     }
/* 038 */     boolean project_isNull_2 = project_exprIsNull_0_0;
/* 039 */     UTF8String project_value_2 = null;
/* 040 */
/* 041 */     if (!project_exprIsNull_0_0) {
/* 042 */       if (project_expr_0_0.isNullAt(1)) {
/* 043 */         project_isNull_2 = true;
/* 044 */       } else {
/* 045 */         project_value_2 = project_expr_0_0.getUTF8String(1);
/* 046 */       }
/* 047 */
/* 048 */     }
/* 049 */     boolean project_isNull_4 = project_exprIsNull_0_0;
/* 050 */     UTF8String project_value_4 = null;
/* 051 */
/* 052 */     if (!project_exprIsNull_0_0) {
/* 053 */       if (project_expr_0_0.isNullAt(2)) {
/* 054 */         project_isNull_4 = true;
/* 055 */       } else {
/* 056 */         project_value_4 = project_expr_0_0.getUTF8String(2);
/* 057 */       }
/* 058 */
/* 059 */     }
/* 060 */     boolean project_isNull_6 = project_exprIsNull_0_0;
/* 061 */     UTF8String project_value_6 = null;
/* 062 */
/* 063 */     if (!project_exprIsNull_0_0) {
/* 064 */       if (project_expr_0_0.isNullAt(3)) {
/* 065 */         project_isNull_6 = true;
/* 066 */       } else {
/* 067 */         project_value_6 = project_expr_0_0.getUTF8String(3);
/* 068 */       }
/* 069 */
/* 070 */     }
/* 071 */     boolean project_isNull_8 = project_exprIsNull_0_0;
/* 072 */     UTF8String project_value_8 = null;
/* 073 */
/* 074 */     if (!project_exprIsNull_0_0) {
/* 075 */       if (project_expr_0_0.isNullAt(4)) {
/* 076 */         project_isNull_8 = true;
/* 077 */       } else {
/* 078 */         project_value_8 = project_expr_0_0.getUTF8String(4);
/* 079 */       }
/* 080 */
/* 081 */     }
/* 082 */     boolean project_isNull_10 = project_exprIsNull_0_0;
/* 083 */     UTF8String project_value_10 = null;
/* 084 */
/* 085 */     if (!project_exprIsNull_0_0) {
/* 086 */       if (project_expr_0_0.isNullAt(5)) {
/* 087 */         project_isNull_10 = true;
/* 088 */       } else {
/* 089 */         project_value_10 = project_expr_0_0.getUTF8String(5);
/* 090 */       }
/* 091 */
/* 092 */     }
/* 093 */     boolean project_isNull_12 = project_exprIsNull_0_0;
/* 094 */     UTF8String project_value_12 = null;
/* 095 */
/* 096 */     if (!project_exprIsNull_0_0) {
/* 097 */       if (project_expr_0_0.isNullAt(6)) {
/* 098 */         project_isNull_12 = true;
/* 099 */       } else {
/* 100 */         project_value_12 = project_expr_0_0.getUTF8String(6);
/* 101 */       }
/* 102 */
/* 103 */     }
/* 104 */     boolean project_isNull_14 = project_exprIsNull_0_0;
/* 105 */     UTF8String project_value_14 = null;
/* 106 */
/* 107 */     if (!project_exprIsNull_0_0) {
/* 108 */       if (project_expr_0_0.isNullAt(7)) {
/* 109 */         project_isNull_14 = true;
/* 110 */       } else {
/* 111 */         project_value_14 = project_expr_0_0.getUTF8String(7);
/* 112 */       }
/* 113 */
/* 114 */     }
/* 115 */     boolean project_isNull_16 = project_exprIsNull_0_0;
/* 116 */     UTF8String project_value_16 = null;
/* 117 */
/* 118 */     if (!project_exprIsNull_0_0) {
/* 119 */       if (project_expr_0_0.isNullAt(8)) {
/* 120 */         project_isNull_16 = true;
/* 121 */       } else {
/* 122 */         project_value_16 = project_expr_0_0.getUTF8String(8);
/* 123 */       }
/* 124 */
/* 125 */     }
/* 126 */     boolean project_isNull_18 = project_exprIsNull_0_0;
/* 127 */     UTF8String project_value_18 = null;
/* 128 */
/* 129 */     if (!project_exprIsNull_0_0) {
/* 130 */       if (project_expr_0_0.isNullAt(9)) {
/* 131 */         project_isNull_18 = true;
/* 132 */       } else {
/* 133 */         project_value_18 = project_expr_0_0.getUTF8String(9);
/* 134 */       }
/* 135 */
/* 136 */     }
/* 137 */     boolean project_isNull_20 = project_exprIsNull_0_0;
/* 138 */     UTF8String project_value_20 = null;
/* 139 */
/* 140 */     if (!project_exprIsNull_0_0) {
/* 141 */       if (project_expr_0_0.isNullAt(10)) {
/* 142 */         project_isNull_20 = true;
/* 143 */       } else {
/* 144 */         project_value_20 = project_expr_0_0.getUTF8String(10);
/* 145 */       }
/* 146 */
/* 147 */     }
/* 148 */     boolean project_isNull_22 = project_exprIsNull_0_0;
/* 149 */     UTF8String project_value_22 = null;
/* 150 */
/* 151 */     if (!project_exprIsNull_0_0) {
/* 152 */       if (project_expr_0_0.isNullAt(11)) {
/* 153 */         project_isNull_22 = true;
/* 154 */       } else {
/* 155 */         project_value_22 = project_expr_0_0.getUTF8String(11);
/* 156 */       }
/* 157 */
/* 158 */     }
/* 159 */     project_mutableStateArray_0[0].reset();
/* 160 */
/* 161 */     project_mutableStateArray_0[0].zeroOutNullBytes();
/* 162 */
/* 163 */     if (project_isNull_0) {
/* 164 */       project_mutableStateArray_0[0].setNullAt(0);
/* 165 */     } else {
/* 166 */       project_mutableStateArray_0[0].write(0, project_value_0);
/* 167 */     }
/* 168 */
/* 169 */     if (project_isNull_2) {
/* 170 */       project_mutableStateArray_0[0].setNullAt(1);
/* 171 */     } else {
/* 172 */       project_mutableStateArray_0[0].write(1, project_value_2);
/* 173 */     }
/* 174 */
/* 175 */     if (project_isNull_4) {
/* 176 */       project_mutableStateArray_0[0].setNullAt(2);
/* 177 */     } else {
/* 178 */       project_mutableStateArray_0[0].write(2, project_value_4);
/* 179 */     }
/* 180 */
/* 181 */     if (project_isNull_6) {
/* 182 */       project_mutableStateArray_0[0].setNullAt(3);
/* 183 */     } else {
/* 184 */       project_mutableStateArray_0[0].write(3, project_value_6);
/* 185 */     }
/* 186 */
/* 187 */     if (project_isNull_8) {
/* 188 */       project_mutableStateArray_0[0].setNullAt(4);
/* 189 */     } else {
/* 190 */       project_mutableStateArray_0[0].write(4, project_value_8);
/* 191 */     }
/* 192 */
/* 193 */     if (project_isNull_10) {
/* 194 */       project_mutableStateArray_0[0].setNullAt(5);
/* 195 */     } else {
/* 196 */       project_mutableStateArray_0[0].write(5, project_value_10);
/* 197 */     }
/* 198 */
/* 199 */     if (project_isNull_12) {
/* 200 */       project_mutableStateArray_0[0].setNullAt(6);
/* 201 */     } else {
/* 202 */       project_mutableStateArray_0[0].write(6, project_value_12);
/* 203 */     }
/* 204 */
/* 205 */     if (project_isNull_14) {
/* 206 */       project_mutableStateArray_0[0].setNullAt(7);
/* 207 */     } else {
/* 208 */       project_mutableStateArray_0[0].write(7, project_value_14);
/* 209 */     }
/* 210 */
/* 211 */     if (project_isNull_16) {
/* 212 */       project_mutableStateArray_0[0].setNullAt(8);
/* 213 */     } else {
/* 214 */       project_mutableStateArray_0[0].write(8, project_value_16);
/* 215 */     }
/* 216 */
/* 217 */     if (project_isNull_18) {
/* 218 */       project_mutableStateArray_0[0].setNullAt(9);
/* 219 */     } else {
/* 220 */       project_mutableStateArray_0[0].write(9, project_value_18);
/* 221 */     }
/* 222 */
/* 223 */     if (project_isNull_20) {
/* 224 */       project_mutableStateArray_0[0].setNullAt(10);
/* 225 */     } else {
/* 226 */       project_mutableStateArray_0[0].write(10, project_value_20);
/* 227 */     }
/* 228 */
/* 229 */     if (project_isNull_22) {
/* 230 */       project_mutableStateArray_0[0].setNullAt(11);
/* 231 */     } else {
/* 232 */       project_mutableStateArray_0[0].write(11, project_value_22);
/* 233 */     }
/* 234 */     append((project_mutableStateArray_0[0].getRow()));
/* 235 */
/* 236 */   }
/* 237 */
/* 238 */   protected void processNext() throws java.io.IOException {
/* 239 */     while ( inputadapter_input_0.hasNext()) {
/* 240 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 241 */
/* 242 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 243 */       InternalRow inputadapter_value_0 = inputadapter_isNull_0 ?
/* 244 */       null : (inputadapter_row_0.getStruct(0, 12));
/* 245 */
/* 246 */       project_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);
/* 247 */       if (shouldStop()) return;
/* 248 */     }
/* 249 */   }
/* 250 */
/* 251 */ }

24/09/12 12:06:00 INFO CodeGenerator: Code generated in 107.869709 ms
24/09/12 12:06:00 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$1
24/09/12 12:06:00 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$1) is now cleaned +++
24/09/12 12:06:00 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted
24/09/12 12:06:00 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
2024-09-12 12:06:00,758 - INFO - Python Server ready to receive messages
2024-09-12 12:06:00,758 - INFO - Received command c on object id p0
2024-09-12 12:06:00,759 - DEBUG - Command to send: c
o57
sparkSession
e

2024-09-12 12:06:00,759 - DEBUG - Answer received: !yro58
2024-09-12 12:06:00,759 - DEBUG - Command to send: r
u
SparkSession$
rj
e

2024-09-12 12:06:00,760 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-09-12 12:06:00,760 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-09-12 12:06:00,761 - DEBUG - Answer received: !yro59
2024-09-12 12:06:00,761 - DEBUG - Command to send: i
java.util.HashMap
e

2024-09-12 12:06:00,761 - DEBUG - Answer received: !yao60
2024-09-12 12:06:00,761 - DEBUG - Command to send: c
o59
applyModifiableSettings
ro58
ro60
e

2024-09-12 12:06:00,762 - DEBUG - Answer received: !yv
2024-09-12 12:06:00,763 - DEBUG - Command to send: c
o57
isEmpty
e

24/09/12 12:06:00 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   private void wholestagecodegen_doConsume_0() throws java.io.IOException {
/* 025 */     append(unsafeRow);
/* 026 */   }
/* 027 */
/* 028 */   protected void processNext() throws java.io.IOException {
/* 029 */     while ( rdd_input_0.hasNext()) {
/* 030 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 032 */       // common sub-expressions
/* 033 */
/* 034 */       wholestagecodegen_doConsume_0();
/* 035 */       if (shouldStop()) return;
/* 036 */     }
/* 037 */   }
/* 038 */
/* 039 */ }

24/09/12 12:06:00 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   private void wholestagecodegen_doConsume_0() throws java.io.IOException {
/* 025 */     append(unsafeRow);
/* 026 */   }
/* 027 */
/* 028 */   protected void processNext() throws java.io.IOException {
/* 029 */     while ( rdd_input_0.hasNext()) {
/* 030 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 032 */       // common sub-expressions
/* 033 */
/* 034 */       wholestagecodegen_doConsume_0();
/* 035 */       if (shouldStop()) return;
/* 036 */     }
/* 037 */   }
/* 038 */
/* 039 */ }

24/09/12 12:06:00 INFO CodeGenerator: Code generated in 9.7185 ms
24/09/12 12:06:00 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted
24/09/12 12:06:00 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
24/09/12 12:06:00 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2
24/09/12 12:06:00 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
24/09/12 12:06:00 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
24/09/12 12:06:00 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
24/09/12 12:06:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/09/12 12:06:00 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 6 took 0.001663 seconds
24/09/12 12:06:00 DEBUG DAGScheduler: Merging stage rdd profiles: Set()
24/09/12 12:06:00 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/09/12 12:06:00 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/09/12 12:06:00 INFO DAGScheduler: Parents of final stage: List()
24/09/12 12:06:00 INFO DAGScheduler: Missing parents: List()
24/09/12 12:06:00 DEBUG DAGScheduler: submitStage(ResultStage 0 (name=start at NativeMethodAccessorImpl.java:0;jobs=0))
24/09/12 12:06:00 DEBUG DAGScheduler: missing: List()
24/09/12 12:06:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/09/12 12:06:00 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)
24/09/12 12:06:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 43.3 KiB, free 434.4 MiB)
24/09/12 12:06:00 DEBUG BlockManager: Put block broadcast_0 locally took 12 ms
24/09/12 12:06:00 DEBUG BlockManager: Putting block broadcast_0 without replication took 13 ms
24/09/12 12:06:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 434.3 MiB)
24/09/12 12:06:00 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:06:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c8faee0cb5df:43645 (size: 16.0 KiB, free: 434.4 MiB)
24/09/12 12:06:00 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
24/09/12 12:06:00 DEBUG BlockManager: Told master about block broadcast_0_piece0
24/09/12 12:06:00 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 7 ms
24/09/12 12:06:00 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 8 ms
24/09/12 12:06:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/09/12 12:06:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/09/12 12:06:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/09/12 12:06:00 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
24/09/12 12:06:00 DEBUG TaskSetManager: Adding pending tasks took 0 ms
24/09/12 12:06:00 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
24/09/12 12:06:00 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
24/09/12 12:06:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.29.0.6, executor 0, partition 0, PROCESS_LOCAL, 15039 bytes) 
24/09/12 12:06:00 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
24/09/12 12:06:00 DEBUG StandaloneSchedulerBackend$StandaloneDriverEndpoint: Launching task 0 on executor id: 0 hostname: 172.29.0.6.
24/09/12 12:06:01 DEBUG TransportServer: New connection accepted for remote address /172.29.0.6:52336.
24/09/12 12:06:01 DEBUG BlockManager: Getting local block broadcast_0_piece0 as bytes
24/09/12 12:06:01 DEBUG BlockManager: Level for block broadcast_0_piece0 is StorageLevel(disk, memory, 1 replicas)
24/09/12 12:06:01 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(0, 172.29.0.6, 40323, None)
24/09/12 12:06:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.29.0.6:40323 (size: 16.0 KiB, free: 434.4 MiB)
2024-09-12 12:06:01,105 - DEBUG - Command to send: m
d
o60
e

2024-09-12 12:06:01,106 - DEBUG - Answer received: !yv
24/09/12 12:06:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1525 ms on 172.29.0.6 (executor 0) (1/1)
24/09/12 12:06:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/09/12 12:06:02 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.662 s
24/09/12 12:06:02 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
24/09/12 12:06:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/12 12:06:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/09/12 12:06:02 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 1.686986 s
2024-09-12 12:06:02,509 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:02,509 - INFO - Processing batch 1
2024-09-12 12:06:02,509 - DEBUG - Command to send: c
o57
showString
i20
i0
bFalse
e

24/09/12 12:06:02 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */     rdd_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( rdd_input_0.hasNext()) {
/* 027 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       // common sub-expressions
/* 030 */
/* 031 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);
/* 032 */       UTF8String rdd_value_0 = rdd_isNull_0 ?
/* 033 */       null : (rdd_row_0.getUTF8String(0));
/* 034 */       UTF8String project_value_0;
/* 035 */       if (rdd_isNull_0) {
/* 036 */         project_value_0 = UTF8String.fromString("NULL");
/* 037 */       } else {
/* 038 */         project_value_0 = rdd_value_0;
/* 039 */       }
/* 040 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);
/* 041 */       UTF8String rdd_value_1 = rdd_isNull_1 ?
/* 042 */       null : (rdd_row_0.getUTF8String(1));
/* 043 */       UTF8String project_value_2;
/* 044 */       if (rdd_isNull_1) {
/* 045 */         project_value_2 = UTF8String.fromString("NULL");
/* 046 */       } else {
/* 047 */         project_value_2 = rdd_value_1;
/* 048 */       }
/* 049 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);
/* 050 */       UTF8String rdd_value_2 = rdd_isNull_2 ?
/* 051 */       null : (rdd_row_0.getUTF8String(2));
/* 052 */       UTF8String project_value_4;
/* 053 */       if (rdd_isNull_2) {
/* 054 */         project_value_4 = UTF8String.fromString("NULL");
/* 055 */       } else {
/* 056 */         project_value_4 = rdd_value_2;
/* 057 */       }
/* 058 */       boolean rdd_isNull_3 = rdd_row_0.isNullAt(3);
/* 059 */       UTF8String rdd_value_3 = rdd_isNull_3 ?
/* 060 */       null : (rdd_row_0.getUTF8String(3));
/* 061 */       UTF8String project_value_6;
/* 062 */       if (rdd_isNull_3) {
/* 063 */         project_value_6 = UTF8String.fromString("NULL");
/* 064 */       } else {
/* 065 */         project_value_6 = rdd_value_3;
/* 066 */       }
/* 067 */       boolean rdd_isNull_4 = rdd_row_0.isNullAt(4);
/* 068 */       UTF8String rdd_value_4 = rdd_isNull_4 ?
/* 069 */       null : (rdd_row_0.getUTF8String(4));
/* 070 */       UTF8String project_value_8;
/* 071 */       if (rdd_isNull_4) {
/* 072 */         project_value_8 = UTF8String.fromString("NULL");
/* 073 */       } else {
/* 074 */         project_value_8 = rdd_value_4;
/* 075 */       }
/* 076 */       boolean rdd_isNull_5 = rdd_row_0.isNullAt(5);
/* 077 */       UTF8String rdd_value_5 = rdd_isNull_5 ?
/* 078 */       null : (rdd_row_0.getUTF8String(5));
/* 079 */       UTF8String project_value_10;
/* 080 */       if (rdd_isNull_5) {
/* 081 */         project_value_10 = UTF8String.fromString("NULL");
/* 082 */       } else {
/* 083 */         project_value_10 = rdd_value_5;
/* 084 */       }
/* 085 */       boolean rdd_isNull_6 = rdd_row_0.isNullAt(6);
/* 086 */       UTF8String rdd_value_6 = rdd_isNull_6 ?
/* 087 */       null : (rdd_row_0.getUTF8String(6));
/* 088 */       UTF8String project_value_12;
/* 089 */       if (rdd_isNull_6) {
/* 090 */         project_value_12 = UTF8String.fromString("NULL");
/* 091 */       } else {
/* 092 */         project_value_12 = rdd_value_6;
/* 093 */       }
/* 094 */       boolean rdd_isNull_7 = rdd_row_0.isNullAt(7);
/* 095 */       UTF8String rdd_value_7 = rdd_isNull_7 ?
/* 096 */       null : (rdd_row_0.getUTF8String(7));
/* 097 */       UTF8String project_value_14;
/* 098 */       if (rdd_isNull_7) {
/* 099 */         project_value_14 = UTF8String.fromString("NULL");
/* 100 */       } else {
/* 101 */         project_value_14 = rdd_value_7;
/* 102 */       }
/* 103 */       boolean rdd_isNull_8 = rdd_row_0.isNullAt(8);
/* 104 */       UTF8String rdd_value_8 = rdd_isNull_8 ?
/* 105 */       null : (rdd_row_0.getUTF8String(8));
/* 106 */       UTF8String project_value_16;
/* 107 */       if (rdd_isNull_8) {
/* 108 */         project_value_16 = UTF8String.fromString("NULL");
/* 109 */       } else {
/* 110 */         project_value_16 = rdd_value_8;
/* 111 */       }
/* 112 */       boolean rdd_isNull_9 = rdd_row_0.isNullAt(9);
/* 113 */       UTF8String rdd_value_9 = rdd_isNull_9 ?
/* 114 */       null : (rdd_row_0.getUTF8String(9));
/* 115 */       UTF8String project_value_18;
/* 116 */       if (rdd_isNull_9) {
/* 117 */         project_value_18 = UTF8String.fromString("NULL");
/* 118 */       } else {
/* 119 */         project_value_18 = rdd_value_9;
/* 120 */       }
/* 121 */       boolean rdd_isNull_10 = rdd_row_0.isNullAt(10);
/* 122 */       UTF8String rdd_value_10 = rdd_isNull_10 ?
/* 123 */       null : (rdd_row_0.getUTF8String(10));
/* 124 */       UTF8String project_value_20;
/* 125 */       if (rdd_isNull_10) {
/* 126 */         project_value_20 = UTF8String.fromString("NULL");
/* 127 */       } else {
/* 128 */         project_value_20 = rdd_value_10;
/* 129 */       }
/* 130 */       boolean rdd_isNull_11 = rdd_row_0.isNullAt(11);
/* 131 */       UTF8String rdd_value_11 = rdd_isNull_11 ?
/* 132 */       null : (rdd_row_0.getUTF8String(11));
/* 133 */       UTF8String project_value_22;
/* 134 */       if (rdd_isNull_11) {
/* 135 */         project_value_22 = UTF8String.fromString("NULL");
/* 136 */       } else {
/* 137 */         project_value_22 = rdd_value_11;
/* 138 */       }
/* 139 */       rdd_mutableStateArray_0[1].reset();
/* 140 */
/* 141 */       rdd_mutableStateArray_0[1].write(0, project_value_0);
/* 142 */
/* 143 */       rdd_mutableStateArray_0[1].write(1, project_value_2);
/* 144 */
/* 145 */       rdd_mutableStateArray_0[1].write(2, project_value_4);
/* 146 */
/* 147 */       rdd_mutableStateArray_0[1].write(3, project_value_6);
/* 148 */
/* 149 */       rdd_mutableStateArray_0[1].write(4, project_value_8);
/* 150 */
/* 151 */       rdd_mutableStateArray_0[1].write(5, project_value_10);
/* 152 */
/* 153 */       rdd_mutableStateArray_0[1].write(6, project_value_12);
/* 154 */
/* 155 */       rdd_mutableStateArray_0[1].write(7, project_value_14);
/* 156 */
/* 157 */       rdd_mutableStateArray_0[1].write(8, project_value_16);
/* 158 */
/* 159 */       rdd_mutableStateArray_0[1].write(9, project_value_18);
/* 160 */
/* 161 */       rdd_mutableStateArray_0[1].write(10, project_value_20);
/* 162 */
/* 163 */       rdd_mutableStateArray_0[1].write(11, project_value_22);
/* 164 */       append((rdd_mutableStateArray_0[1].getRow()));
/* 165 */       if (shouldStop()) return;
/* 166 */     }
/* 167 */   }
/* 168 */
/* 169 */ }

24/09/12 12:06:02 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */     rdd_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( rdd_input_0.hasNext()) {
/* 027 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       // common sub-expressions
/* 030 */
/* 031 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);
/* 032 */       UTF8String rdd_value_0 = rdd_isNull_0 ?
/* 033 */       null : (rdd_row_0.getUTF8String(0));
/* 034 */       UTF8String project_value_0;
/* 035 */       if (rdd_isNull_0) {
/* 036 */         project_value_0 = UTF8String.fromString("NULL");
/* 037 */       } else {
/* 038 */         project_value_0 = rdd_value_0;
/* 039 */       }
/* 040 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);
/* 041 */       UTF8String rdd_value_1 = rdd_isNull_1 ?
/* 042 */       null : (rdd_row_0.getUTF8String(1));
/* 043 */       UTF8String project_value_2;
/* 044 */       if (rdd_isNull_1) {
/* 045 */         project_value_2 = UTF8String.fromString("NULL");
/* 046 */       } else {
/* 047 */         project_value_2 = rdd_value_1;
/* 048 */       }
/* 049 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);
/* 050 */       UTF8String rdd_value_2 = rdd_isNull_2 ?
/* 051 */       null : (rdd_row_0.getUTF8String(2));
/* 052 */       UTF8String project_value_4;
/* 053 */       if (rdd_isNull_2) {
/* 054 */         project_value_4 = UTF8String.fromString("NULL");
/* 055 */       } else {
/* 056 */         project_value_4 = rdd_value_2;
/* 057 */       }
/* 058 */       boolean rdd_isNull_3 = rdd_row_0.isNullAt(3);
/* 059 */       UTF8String rdd_value_3 = rdd_isNull_3 ?
/* 060 */       null : (rdd_row_0.getUTF8String(3));
/* 061 */       UTF8String project_value_6;
/* 062 */       if (rdd_isNull_3) {
/* 063 */         project_value_6 = UTF8String.fromString("NULL");
/* 064 */       } else {
/* 065 */         project_value_6 = rdd_value_3;
/* 066 */       }
/* 067 */       boolean rdd_isNull_4 = rdd_row_0.isNullAt(4);
/* 068 */       UTF8String rdd_value_4 = rdd_isNull_4 ?
/* 069 */       null : (rdd_row_0.getUTF8String(4));
/* 070 */       UTF8String project_value_8;
/* 071 */       if (rdd_isNull_4) {
/* 072 */         project_value_8 = UTF8String.fromString("NULL");
/* 073 */       } else {
/* 074 */         project_value_8 = rdd_value_4;
/* 075 */       }
/* 076 */       boolean rdd_isNull_5 = rdd_row_0.isNullAt(5);
/* 077 */       UTF8String rdd_value_5 = rdd_isNull_5 ?
/* 078 */       null : (rdd_row_0.getUTF8String(5));
/* 079 */       UTF8String project_value_10;
/* 080 */       if (rdd_isNull_5) {
/* 081 */         project_value_10 = UTF8String.fromString("NULL");
/* 082 */       } else {
/* 083 */         project_value_10 = rdd_value_5;
/* 084 */       }
/* 085 */       boolean rdd_isNull_6 = rdd_row_0.isNullAt(6);
/* 086 */       UTF8String rdd_value_6 = rdd_isNull_6 ?
/* 087 */       null : (rdd_row_0.getUTF8String(6));
/* 088 */       UTF8String project_value_12;
/* 089 */       if (rdd_isNull_6) {
/* 090 */         project_value_12 = UTF8String.fromString("NULL");
/* 091 */       } else {
/* 092 */         project_value_12 = rdd_value_6;
/* 093 */       }
/* 094 */       boolean rdd_isNull_7 = rdd_row_0.isNullAt(7);
/* 095 */       UTF8String rdd_value_7 = rdd_isNull_7 ?
/* 096 */       null : (rdd_row_0.getUTF8String(7));
/* 097 */       UTF8String project_value_14;
/* 098 */       if (rdd_isNull_7) {
/* 099 */         project_value_14 = UTF8String.fromString("NULL");
/* 100 */       } else {
/* 101 */         project_value_14 = rdd_value_7;
/* 102 */       }
/* 103 */       boolean rdd_isNull_8 = rdd_row_0.isNullAt(8);
/* 104 */       UTF8String rdd_value_8 = rdd_isNull_8 ?
/* 105 */       null : (rdd_row_0.getUTF8String(8));
/* 106 */       UTF8String project_value_16;
/* 107 */       if (rdd_isNull_8) {
/* 108 */         project_value_16 = UTF8String.fromString("NULL");
/* 109 */       } else {
/* 110 */         project_value_16 = rdd_value_8;
/* 111 */       }
/* 112 */       boolean rdd_isNull_9 = rdd_row_0.isNullAt(9);
/* 113 */       UTF8String rdd_value_9 = rdd_isNull_9 ?
/* 114 */       null : (rdd_row_0.getUTF8String(9));
/* 115 */       UTF8String project_value_18;
/* 116 */       if (rdd_isNull_9) {
/* 117 */         project_value_18 = UTF8String.fromString("NULL");
/* 118 */       } else {
/* 119 */         project_value_18 = rdd_value_9;
/* 120 */       }
/* 121 */       boolean rdd_isNull_10 = rdd_row_0.isNullAt(10);
/* 122 */       UTF8String rdd_value_10 = rdd_isNull_10 ?
/* 123 */       null : (rdd_row_0.getUTF8String(10));
/* 124 */       UTF8String project_value_20;
/* 125 */       if (rdd_isNull_10) {
/* 126 */         project_value_20 = UTF8String.fromString("NULL");
/* 127 */       } else {
/* 128 */         project_value_20 = rdd_value_10;
/* 129 */       }
/* 130 */       boolean rdd_isNull_11 = rdd_row_0.isNullAt(11);
/* 131 */       UTF8String rdd_value_11 = rdd_isNull_11 ?
/* 132 */       null : (rdd_row_0.getUTF8String(11));
/* 133 */       UTF8String project_value_22;
/* 134 */       if (rdd_isNull_11) {
/* 135 */         project_value_22 = UTF8String.fromString("NULL");
/* 136 */       } else {
/* 137 */         project_value_22 = rdd_value_11;
/* 138 */       }
/* 139 */       rdd_mutableStateArray_0[1].reset();
/* 140 */
/* 141 */       rdd_mutableStateArray_0[1].write(0, project_value_0);
/* 142 */
/* 143 */       rdd_mutableStateArray_0[1].write(1, project_value_2);
/* 144 */
/* 145 */       rdd_mutableStateArray_0[1].write(2, project_value_4);
/* 146 */
/* 147 */       rdd_mutableStateArray_0[1].write(3, project_value_6);
/* 148 */
/* 149 */       rdd_mutableStateArray_0[1].write(4, project_value_8);
/* 150 */
/* 151 */       rdd_mutableStateArray_0[1].write(5, project_value_10);
/* 152 */
/* 153 */       rdd_mutableStateArray_0[1].write(6, project_value_12);
/* 154 */
/* 155 */       rdd_mutableStateArray_0[1].write(7, project_value_14);
/* 156 */
/* 157 */       rdd_mutableStateArray_0[1].write(8, project_value_16);
/* 158 */
/* 159 */       rdd_mutableStateArray_0[1].write(9, project_value_18);
/* 160 */
/* 161 */       rdd_mutableStateArray_0[1].write(10, project_value_20);
/* 162 */
/* 163 */       rdd_mutableStateArray_0[1].write(11, project_value_22);
/* 164 */       append((rdd_mutableStateArray_0[1].getRow()));
/* 165 */       if (shouldStop()) return;
/* 166 */     }
/* 167 */   }
/* 168 */
/* 169 */ }

24/09/12 12:06:02 INFO CodeGenerator: Code generated in 24.9985 ms
24/09/12 12:06:02 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted
24/09/12 12:06:02 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
24/09/12 12:06:02 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2
24/09/12 12:06:02 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
24/09/12 12:06:02 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
24/09/12 12:06:02 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
24/09/12 12:06:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/09/12 12:06:02 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 8 took 0.000105 seconds
24/09/12 12:06:02 DEBUG DAGScheduler: Merging stage rdd profiles: Set()
24/09/12 12:06:02 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/09/12 12:06:02 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/09/12 12:06:02 INFO DAGScheduler: Parents of final stage: List()
24/09/12 12:06:02 INFO DAGScheduler: Missing parents: List()
24/09/12 12:06:02 DEBUG DAGScheduler: submitStage(ResultStage 1 (name=start at NativeMethodAccessorImpl.java:0;jobs=1))
24/09/12 12:06:02 DEBUG DAGScheduler: missing: List()
24/09/12 12:06:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/09/12 12:06:02 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)
24/09/12 12:06:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 47.1 KiB, free 434.3 MiB)
24/09/12 12:06:02 DEBUG BlockManager: Put block broadcast_1 locally took 0 ms
24/09/12 12:06:02 DEBUG BlockManager: Putting block broadcast_1 without replication took 0 ms
24/09/12 12:06:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 434.3 MiB)
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(39)
24/09/12 12:06:02 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:06:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c8faee0cb5df:43645 (size: 16.8 KiB, free: 434.4 MiB)
24/09/12 12:06:02 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
24/09/12 12:06:02 DEBUG BlockManager: Told master about block broadcast_1_piece0
24/09/12 12:06:02 DEBUG BlockManager: Put block broadcast_1_piece0 locally took 0 ms
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 39
24/09/12 12:06:02 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took 0 ms
24/09/12 12:06:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 39
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(15)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 15
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 15
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(36)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 36
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 36
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(3)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 3
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 3
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(25)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 25
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 25
24/09/12 12:06:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(18)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 18
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 18
24/09/12 12:06:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(30)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 30
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 30
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(16)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 16
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 16
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(38)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 38
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 38
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)
24/09/12 12:06:02 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0
24/09/12 12:06:02 DEBUG TaskSetManager: Adding pending tasks took 0 ms
24/09/12 12:06:02 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY
24/09/12 12:06:02 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning broadcast 0
24/09/12 12:06:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.29.0.6, executor 0, partition 0, PROCESS_LOCAL, 15039 bytes) 
24/09/12 12:06:02 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
24/09/12 12:06:02 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0
24/09/12 12:06:02 DEBUG StandaloneSchedulerBackend$StandaloneDriverEndpoint: Launching task 1 on executor id: 0 hostname: 172.29.0.6.
24/09/12 12:06:02 DEBUG BlockManagerStorageEndpoint: removing broadcast 0
24/09/12 12:06:02 DEBUG BlockManager: Removing broadcast 0
24/09/12 12:06:02 DEBUG BlockManager: Removing block broadcast_0_piece0
24/09/12 12:06:02 DEBUG MemoryStore: Block broadcast_0_piece0 of size 16356 dropped from memory (free 455391596)
24/09/12 12:06:02 DEBUG BlockManager: Getting local block broadcast_1_piece0 as bytes
24/09/12 12:06:02 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(0, 172.29.0.6, 40323, None)
24/09/12 12:06:02 DEBUG BlockManager: Level for block broadcast_1_piece0 is StorageLevel(disk, memory, 1 replicas)
24/09/12 12:06:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.29.0.6:40323 in memory (size: 16.0 KiB, free: 434.4 MiB)
24/09/12 12:06:02 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:06:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c8faee0cb5df:43645 in memory (size: 16.0 KiB, free: 434.4 MiB)
24/09/12 12:06:02 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
24/09/12 12:06:02 DEBUG BlockManager: Told master about block broadcast_0_piece0
24/09/12 12:06:02 DEBUG BlockManager: Removing block broadcast_0
24/09/12 12:06:02 DEBUG MemoryStore: Block broadcast_0 of size 44352 dropped from memory (free 455435948)
24/09/12 12:06:02 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 0, response is 0
24/09/12 12:06:02 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to c8faee0cb5df:38537
24/09/12 12:06:02 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(0, 172.29.0.6, 40323, None)
24/09/12 12:06:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.29.0.6:40323 (size: 16.8 KiB, free: 434.4 MiB)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned broadcast 0
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(28)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 28
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 28
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(23)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 23
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 23
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(21)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 21
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 21
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(6)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 6
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 6
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(32)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 32
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 32
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(26)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 26
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 26
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(33)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 33
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 33
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(27)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 27
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 27
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(24)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 24
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 24
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(14)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 14
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 14
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(10)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 10
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 10
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(5)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 5
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 5
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(4)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 4
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 4
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(8)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 8
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 8
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(29)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 29
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 29
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(34)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 34
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 34
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(19)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 19
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 19
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(12)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 12
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 12
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(13)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 13
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 13
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(20)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 20
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 20
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(22)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 22
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 22
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(7)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 7
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 7
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(9)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 9
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 9
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(31)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 31
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 31
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(35)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 35
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 35
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(37)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 37
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 37
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(11)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 11
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 11
24/09/12 12:06:02 DEBUG ContextCleaner: Got cleaning task CleanAccum(17)
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaning accumulator 17
24/09/12 12:06:02 DEBUG ContextCleaner: Cleaned accumulator 17
24/09/12 12:06:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 586 ms on 172.29.0.6 (executor 0) (1/1)
24/09/12 12:06:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/09/12 12:06:03 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
24/09/12 12:06:03 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
24/09/12 12:06:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/12 12:06:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/09/12 12:06:03 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.602955 s
24/09/12 12:06:03 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, input[9, string, false].toString, input[10, string, false].toString, input[11, string, false].toString, StructField(toprettystring(id),StringType,false), StructField(toprettystring(first_name),StringType,false), StructField(toprettystring(last_name),StringType,false), StructField(toprettystring(gender),StringType,false), StructField(toprettystring(address),StringType,false), StructField(toprettystring(post_code),StringType,false), StructField(toprettystring(email),StringType,false), StructField(toprettystring(username),StringType,false), StructField(toprettystring(dob),StringType,false), StructField(toprettystring(registered_date),StringType,false), StructField(toprettystring(phone),StringType,false), StructField(toprettystring(picture),StringType,false)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[12];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     createExternalRow_0_3(i, values_0);
/* 028 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 029 */     if (false) {
/* 030 */       mutableRow.setNullAt(0);
/* 031 */     } else {
/* 032 */
/* 033 */       mutableRow.update(0, value_0);
/* 034 */     }
/* 035 */
/* 036 */     return mutableRow;
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 041 */
/* 042 */     UTF8String value_14 = i.getUTF8String(6);
/* 043 */     boolean isNull_13 = true;
/* 044 */     java.lang.String value_13 = null;
/* 045 */     isNull_13 = false;
/* 046 */     if (!isNull_13) {
/* 047 */
/* 048 */       Object funcResult_6 = null;
/* 049 */       funcResult_6 = value_14.toString();
/* 050 */       value_13 = (java.lang.String) funcResult_6;
/* 051 */
/* 052 */     }
/* 053 */     if (isNull_13) {
/* 054 */       values_0[6] = null;
/* 055 */     } else {
/* 056 */       values_0[6] = value_13;
/* 057 */     }
/* 058 */
/* 059 */     UTF8String value_16 = i.getUTF8String(7);
/* 060 */     boolean isNull_15 = true;
/* 061 */     java.lang.String value_15 = null;
/* 062 */     isNull_15 = false;
/* 063 */     if (!isNull_15) {
/* 064 */
/* 065 */       Object funcResult_7 = null;
/* 066 */       funcResult_7 = value_16.toString();
/* 067 */       value_15 = (java.lang.String) funcResult_7;
/* 068 */
/* 069 */     }
/* 070 */     if (isNull_15) {
/* 071 */       values_0[7] = null;
/* 072 */     } else {
/* 073 */       values_0[7] = value_15;
/* 074 */     }
/* 075 */
/* 076 */     UTF8String value_18 = i.getUTF8String(8);
/* 077 */     boolean isNull_17 = true;
/* 078 */     java.lang.String value_17 = null;
/* 079 */     isNull_17 = false;
/* 080 */     if (!isNull_17) {
/* 081 */
/* 082 */       Object funcResult_8 = null;
/* 083 */       funcResult_8 = value_18.toString();
/* 084 */       value_17 = (java.lang.String) funcResult_8;
/* 085 */
/* 086 */     }
/* 087 */     if (isNull_17) {
/* 088 */       values_0[8] = null;
/* 089 */     } else {
/* 090 */       values_0[8] = value_17;
/* 091 */     }
/* 092 */
/* 093 */   }
/* 094 */
/* 095 */
/* 096 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 097 */
/* 098 */     UTF8String value_8 = i.getUTF8String(3);
/* 099 */     boolean isNull_7 = true;
/* 100 */     java.lang.String value_7 = null;
/* 101 */     isNull_7 = false;
/* 102 */     if (!isNull_7) {
/* 103 */
/* 104 */       Object funcResult_3 = null;
/* 105 */       funcResult_3 = value_8.toString();
/* 106 */       value_7 = (java.lang.String) funcResult_3;
/* 107 */
/* 108 */     }
/* 109 */     if (isNull_7) {
/* 110 */       values_0[3] = null;
/* 111 */     } else {
/* 112 */       values_0[3] = value_7;
/* 113 */     }
/* 114 */
/* 115 */     UTF8String value_10 = i.getUTF8String(4);
/* 116 */     boolean isNull_9 = true;
/* 117 */     java.lang.String value_9 = null;
/* 118 */     isNull_9 = false;
/* 119 */     if (!isNull_9) {
/* 120 */
/* 121 */       Object funcResult_4 = null;
/* 122 */       funcResult_4 = value_10.toString();
/* 123 */       value_9 = (java.lang.String) funcResult_4;
/* 124 */
/* 125 */     }
/* 126 */     if (isNull_9) {
/* 127 */       values_0[4] = null;
/* 128 */     } else {
/* 129 */       values_0[4] = value_9;
/* 130 */     }
/* 131 */
/* 132 */     UTF8String value_12 = i.getUTF8String(5);
/* 133 */     boolean isNull_11 = true;
/* 134 */     java.lang.String value_11 = null;
/* 135 */     isNull_11 = false;
/* 136 */     if (!isNull_11) {
/* 137 */
/* 138 */       Object funcResult_5 = null;
/* 139 */       funcResult_5 = value_12.toString();
/* 140 */       value_11 = (java.lang.String) funcResult_5;
/* 141 */
/* 142 */     }
/* 143 */     if (isNull_11) {
/* 144 */       values_0[5] = null;
/* 145 */     } else {
/* 146 */       values_0[5] = value_11;
/* 147 */     }
/* 148 */
/* 149 */   }
/* 150 */
/* 151 */
/* 152 */   private void createExternalRow_0_3(InternalRow i, Object[] values_0) {
/* 153 */
/* 154 */     UTF8String value_20 = i.getUTF8String(9);
/* 155 */     boolean isNull_19 = true;
/* 156 */     java.lang.String value_19 = null;
/* 157 */     isNull_19 = false;
/* 158 */     if (!isNull_19) {
/* 159 */
/* 160 */       Object funcResult_9 = null;
/* 161 */       funcResult_9 = value_20.toString();
/* 162 */       value_19 = (java.lang.String) funcResult_9;
/* 163 */
/* 164 */     }
/* 165 */     if (isNull_19) {
/* 166 */       values_0[9] = null;
/* 167 */     } else {
/* 168 */       values_0[9] = value_19;
/* 169 */     }
/* 170 */
/* 171 */     UTF8String value_22 = i.getUTF8String(10);
/* 172 */     boolean isNull_21 = true;
/* 173 */     java.lang.String value_21 = null;
/* 174 */     isNull_21 = false;
/* 175 */     if (!isNull_21) {
/* 176 */
/* 177 */       Object funcResult_10 = null;
/* 178 */       funcResult_10 = value_22.toString();
/* 179 */       value_21 = (java.lang.String) funcResult_10;
/* 180 */
/* 181 */     }
/* 182 */     if (isNull_21) {
/* 183 */       values_0[10] = null;
/* 184 */     } else {
/* 185 */       values_0[10] = value_21;
/* 186 */     }
/* 187 */
/* 188 */     UTF8String value_24 = i.getUTF8String(11);
/* 189 */     boolean isNull_23 = true;
/* 190 */     java.lang.String value_23 = null;
/* 191 */     isNull_23 = false;
/* 192 */     if (!isNull_23) {
/* 193 */
/* 194 */       Object funcResult_11 = null;
/* 195 */       funcResult_11 = value_24.toString();
/* 196 */       value_23 = (java.lang.String) funcResult_11;
/* 197 */
/* 198 */     }
/* 199 */     if (isNull_23) {
/* 200 */       values_0[11] = null;
/* 201 */     } else {
/* 202 */       values_0[11] = value_23;
/* 203 */     }
/* 204 */
/* 205 */   }
/* 206 */
/* 207 */
/* 208 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 209 */
/* 210 */     UTF8String value_2 = i.getUTF8String(0);
/* 211 */     boolean isNull_1 = true;
/* 212 */     java.lang.String value_1 = null;
/* 213 */     isNull_1 = false;
/* 214 */     if (!isNull_1) {
/* 215 */
/* 216 */       Object funcResult_0 = null;
/* 217 */       funcResult_0 = value_2.toString();
/* 218 */       value_1 = (java.lang.String) funcResult_0;
/* 219 */
/* 220 */     }
/* 221 */     if (isNull_1) {
/* 222 */       values_0[0] = null;
/* 223 */     } else {
/* 224 */       values_0[0] = value_1;
/* 225 */     }
/* 226 */
/* 227 */     UTF8String value_4 = i.getUTF8String(1);
/* 228 */     boolean isNull_3 = true;
/* 229 */     java.lang.String value_3 = null;
/* 230 */     isNull_3 = false;
/* 231 */     if (!isNull_3) {
/* 232 */
/* 233 */       Object funcResult_1 = null;
/* 234 */       funcResult_1 = value_4.toString();
/* 235 */       value_3 = (java.lang.String) funcResult_1;
/* 236 */
/* 237 */     }
/* 238 */     if (isNull_3) {
/* 239 */       values_0[1] = null;
/* 240 */     } else {
/* 241 */       values_0[1] = value_3;
/* 242 */     }
/* 243 */
/* 244 */     UTF8String value_6 = i.getUTF8String(2);
/* 245 */     boolean isNull_5 = true;
/* 246 */     java.lang.String value_5 = null;
/* 247 */     isNull_5 = false;
/* 248 */     if (!isNull_5) {
/* 249 */
/* 250 */       Object funcResult_2 = null;
/* 251 */       funcResult_2 = value_6.toString();
/* 252 */       value_5 = (java.lang.String) funcResult_2;
/* 253 */
/* 254 */     }
/* 255 */     if (isNull_5) {
/* 256 */       values_0[2] = null;
/* 257 */     } else {
/* 258 */       values_0[2] = value_5;
/* 259 */     }
/* 260 */
/* 261 */   }
/* 262 */
/* 263 */ }

24/09/12 12:06:03 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[12];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     createExternalRow_0_3(i, values_0);
/* 028 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 029 */     if (false) {
/* 030 */       mutableRow.setNullAt(0);
/* 031 */     } else {
/* 032 */
/* 033 */       mutableRow.update(0, value_0);
/* 034 */     }
/* 035 */
/* 036 */     return mutableRow;
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 041 */
/* 042 */     UTF8String value_14 = i.getUTF8String(6);
/* 043 */     boolean isNull_13 = true;
/* 044 */     java.lang.String value_13 = null;
/* 045 */     isNull_13 = false;
/* 046 */     if (!isNull_13) {
/* 047 */
/* 048 */       Object funcResult_6 = null;
/* 049 */       funcResult_6 = value_14.toString();
/* 050 */       value_13 = (java.lang.String) funcResult_6;
/* 051 */
/* 052 */     }
/* 053 */     if (isNull_13) {
/* 054 */       values_0[6] = null;
/* 055 */     } else {
/* 056 */       values_0[6] = value_13;
/* 057 */     }
/* 058 */
/* 059 */     UTF8String value_16 = i.getUTF8String(7);
/* 060 */     boolean isNull_15 = true;
/* 061 */     java.lang.String value_15 = null;
/* 062 */     isNull_15 = false;
/* 063 */     if (!isNull_15) {
/* 064 */
/* 065 */       Object funcResult_7 = null;
/* 066 */       funcResult_7 = value_16.toString();
/* 067 */       value_15 = (java.lang.String) funcResult_7;
/* 068 */
/* 069 */     }
/* 070 */     if (isNull_15) {
/* 071 */       values_0[7] = null;
/* 072 */     } else {
/* 073 */       values_0[7] = value_15;
/* 074 */     }
/* 075 */
/* 076 */     UTF8String value_18 = i.getUTF8String(8);
/* 077 */     boolean isNull_17 = true;
/* 078 */     java.lang.String value_17 = null;
/* 079 */     isNull_17 = false;
/* 080 */     if (!isNull_17) {
/* 081 */
/* 082 */       Object funcResult_8 = null;
/* 083 */       funcResult_8 = value_18.toString();
/* 084 */       value_17 = (java.lang.String) funcResult_8;
/* 085 */
/* 086 */     }
/* 087 */     if (isNull_17) {
/* 088 */       values_0[8] = null;
/* 089 */     } else {
/* 090 */       values_0[8] = value_17;
/* 091 */     }
/* 092 */
/* 093 */   }
/* 094 */
/* 095 */
/* 096 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 097 */
/* 098 */     UTF8String value_8 = i.getUTF8String(3);
/* 099 */     boolean isNull_7 = true;
/* 100 */     java.lang.String value_7 = null;
/* 101 */     isNull_7 = false;
/* 102 */     if (!isNull_7) {
/* 103 */
/* 104 */       Object funcResult_3 = null;
/* 105 */       funcResult_3 = value_8.toString();
/* 106 */       value_7 = (java.lang.String) funcResult_3;
/* 107 */
/* 108 */     }
/* 109 */     if (isNull_7) {
/* 110 */       values_0[3] = null;
/* 111 */     } else {
/* 112 */       values_0[3] = value_7;
/* 113 */     }
/* 114 */
/* 115 */     UTF8String value_10 = i.getUTF8String(4);
/* 116 */     boolean isNull_9 = true;
/* 117 */     java.lang.String value_9 = null;
/* 118 */     isNull_9 = false;
/* 119 */     if (!isNull_9) {
/* 120 */
/* 121 */       Object funcResult_4 = null;
/* 122 */       funcResult_4 = value_10.toString();
/* 123 */       value_9 = (java.lang.String) funcResult_4;
/* 124 */
/* 125 */     }
/* 126 */     if (isNull_9) {
/* 127 */       values_0[4] = null;
/* 128 */     } else {
/* 129 */       values_0[4] = value_9;
/* 130 */     }
/* 131 */
/* 132 */     UTF8String value_12 = i.getUTF8String(5);
/* 133 */     boolean isNull_11 = true;
/* 134 */     java.lang.String value_11 = null;
/* 135 */     isNull_11 = false;
/* 136 */     if (!isNull_11) {
/* 137 */
/* 138 */       Object funcResult_5 = null;
/* 139 */       funcResult_5 = value_12.toString();
/* 140 */       value_11 = (java.lang.String) funcResult_5;
/* 141 */
/* 142 */     }
/* 143 */     if (isNull_11) {
/* 144 */       values_0[5] = null;
/* 145 */     } else {
/* 146 */       values_0[5] = value_11;
/* 147 */     }
/* 148 */
/* 149 */   }
/* 150 */
/* 151 */
/* 152 */   private void createExternalRow_0_3(InternalRow i, Object[] values_0) {
/* 153 */
/* 154 */     UTF8String value_20 = i.getUTF8String(9);
/* 155 */     boolean isNull_19 = true;
/* 156 */     java.lang.String value_19 = null;
/* 157 */     isNull_19 = false;
/* 158 */     if (!isNull_19) {
/* 159 */
/* 160 */       Object funcResult_9 = null;
/* 161 */       funcResult_9 = value_20.toString();
/* 162 */       value_19 = (java.lang.String) funcResult_9;
/* 163 */
/* 164 */     }
/* 165 */     if (isNull_19) {
/* 166 */       values_0[9] = null;
/* 167 */     } else {
/* 168 */       values_0[9] = value_19;
/* 169 */     }
/* 170 */
/* 171 */     UTF8String value_22 = i.getUTF8String(10);
/* 172 */     boolean isNull_21 = true;
/* 173 */     java.lang.String value_21 = null;
/* 174 */     isNull_21 = false;
/* 175 */     if (!isNull_21) {
/* 176 */
/* 177 */       Object funcResult_10 = null;
/* 178 */       funcResult_10 = value_22.toString();
/* 179 */       value_21 = (java.lang.String) funcResult_10;
/* 180 */
/* 181 */     }
/* 182 */     if (isNull_21) {
/* 183 */       values_0[10] = null;
/* 184 */     } else {
/* 185 */       values_0[10] = value_21;
/* 186 */     }
/* 187 */
/* 188 */     UTF8String value_24 = i.getUTF8String(11);
/* 189 */     boolean isNull_23 = true;
/* 190 */     java.lang.String value_23 = null;
/* 191 */     isNull_23 = false;
/* 192 */     if (!isNull_23) {
/* 193 */
/* 194 */       Object funcResult_11 = null;
/* 195 */       funcResult_11 = value_24.toString();
/* 196 */       value_23 = (java.lang.String) funcResult_11;
/* 197 */
/* 198 */     }
/* 199 */     if (isNull_23) {
/* 200 */       values_0[11] = null;
/* 201 */     } else {
/* 202 */       values_0[11] = value_23;
/* 203 */     }
/* 204 */
/* 205 */   }
/* 206 */
/* 207 */
/* 208 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 209 */
/* 210 */     UTF8String value_2 = i.getUTF8String(0);
/* 211 */     boolean isNull_1 = true;
/* 212 */     java.lang.String value_1 = null;
/* 213 */     isNull_1 = false;
/* 214 */     if (!isNull_1) {
/* 215 */
/* 216 */       Object funcResult_0 = null;
/* 217 */       funcResult_0 = value_2.toString();
/* 218 */       value_1 = (java.lang.String) funcResult_0;
/* 219 */
/* 220 */     }
/* 221 */     if (isNull_1) {
/* 222 */       values_0[0] = null;
/* 223 */     } else {
/* 224 */       values_0[0] = value_1;
/* 225 */     }
/* 226 */
/* 227 */     UTF8String value_4 = i.getUTF8String(1);
/* 228 */     boolean isNull_3 = true;
/* 229 */     java.lang.String value_3 = null;
/* 230 */     isNull_3 = false;
/* 231 */     if (!isNull_3) {
/* 232 */
/* 233 */       Object funcResult_1 = null;
/* 234 */       funcResult_1 = value_4.toString();
/* 235 */       value_3 = (java.lang.String) funcResult_1;
/* 236 */
/* 237 */     }
/* 238 */     if (isNull_3) {
/* 239 */       values_0[1] = null;
/* 240 */     } else {
/* 241 */       values_0[1] = value_3;
/* 242 */     }
/* 243 */
/* 244 */     UTF8String value_6 = i.getUTF8String(2);
/* 245 */     boolean isNull_5 = true;
/* 246 */     java.lang.String value_5 = null;
/* 247 */     isNull_5 = false;
/* 248 */     if (!isNull_5) {
/* 249 */
/* 250 */       Object funcResult_2 = null;
/* 251 */       funcResult_2 = value_6.toString();
/* 252 */       value_5 = (java.lang.String) funcResult_2;
/* 253 */
/* 254 */     }
/* 255 */     if (isNull_5) {
/* 256 */       values_0[2] = null;
/* 257 */     } else {
/* 258 */       values_0[2] = value_5;
/* 259 */     }
/* 260 */
/* 261 */   }
/* 262 */
/* 263 */ }

24/09/12 12:06:03 INFO CodeGenerator: Code generated in 26.079959 ms
2024-09-12 12:06:03,690 - DEBUG - Answer received: !ys+-----+----------+---------+------+-------+---------+-----+--------+----+---------------+-----+-------+\n|id   |first_name|last_name|gender|address|post_code|email|username|dob |registered_date|phone|picture|\n+-----+----------+---------+------+-------+---------+-----+--------+----+---------------+-----+-------+\n|test1|John      |Doe      |NULL  |NULL   |NULL     |NULL |NULL    |NULL|NULL           |NULL |NULL   |\n+-----+----------+---------+------+-------+---------+-----+--------+----+---------------+-----+-------+\n
+-----+----------+---------+------+-------+---------+-----+--------+----+---------------+-----+-------+
|id   |first_name|last_name|gender|address|post_code|email|username|dob |registered_date|phone|picture|
+-----+----------+---------+------+-------+---------+-----+--------+----+---------------+-----+-------+
|test1|John      |Doe      |NULL  |NULL   |NULL     |NULL |NULL    |NULL|NULL           |NULL |NULL   |
+-----+----------+---------+------+-------+---------+-----+--------+----+---------------+-----+-------+

2024-09-12 12:06:03,690 - INFO - Batch content: None
2024-09-12 12:06:03,692 - DEBUG - Command to send: c
o20
setCallSite
scall at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
e

2024-09-12 12:06:03,693 - DEBUG - Answer received: !yv
2024-09-12 12:06:03,693 - DEBUG - Command to send: c
o57
collectToPython
e

24/09/12 12:06:03 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( rdd_input_0.hasNext()) {
/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);
/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?
/* 030 */       null : (rdd_row_0.getUTF8String(0));
/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);
/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?
/* 033 */       null : (rdd_row_0.getUTF8String(1));
/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);
/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?
/* 036 */       null : (rdd_row_0.getUTF8String(2));
/* 037 */       boolean rdd_isNull_3 = rdd_row_0.isNullAt(3);
/* 038 */       UTF8String rdd_value_3 = rdd_isNull_3 ?
/* 039 */       null : (rdd_row_0.getUTF8String(3));
/* 040 */       boolean rdd_isNull_4 = rdd_row_0.isNullAt(4);
/* 041 */       UTF8String rdd_value_4 = rdd_isNull_4 ?
/* 042 */       null : (rdd_row_0.getUTF8String(4));
/* 043 */       boolean rdd_isNull_5 = rdd_row_0.isNullAt(5);
/* 044 */       UTF8String rdd_value_5 = rdd_isNull_5 ?
/* 045 */       null : (rdd_row_0.getUTF8String(5));
/* 046 */       boolean rdd_isNull_6 = rdd_row_0.isNullAt(6);
/* 047 */       UTF8String rdd_value_6 = rdd_isNull_6 ?
/* 048 */       null : (rdd_row_0.getUTF8String(6));
/* 049 */       boolean rdd_isNull_7 = rdd_row_0.isNullAt(7);
/* 050 */       UTF8String rdd_value_7 = rdd_isNull_7 ?
/* 051 */       null : (rdd_row_0.getUTF8String(7));
/* 052 */       boolean rdd_isNull_8 = rdd_row_0.isNullAt(8);
/* 053 */       UTF8String rdd_value_8 = rdd_isNull_8 ?
/* 054 */       null : (rdd_row_0.getUTF8String(8));
/* 055 */       boolean rdd_isNull_9 = rdd_row_0.isNullAt(9);
/* 056 */       UTF8String rdd_value_9 = rdd_isNull_9 ?
/* 057 */       null : (rdd_row_0.getUTF8String(9));
/* 058 */       boolean rdd_isNull_10 = rdd_row_0.isNullAt(10);
/* 059 */       UTF8String rdd_value_10 = rdd_isNull_10 ?
/* 060 */       null : (rdd_row_0.getUTF8String(10));
/* 061 */       boolean rdd_isNull_11 = rdd_row_0.isNullAt(11);
/* 062 */       UTF8String rdd_value_11 = rdd_isNull_11 ?
/* 063 */       null : (rdd_row_0.getUTF8String(11));
/* 064 */       rdd_mutableStateArray_0[0].reset();
/* 065 */
/* 066 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();
/* 067 */
/* 068 */       if (rdd_isNull_0) {
/* 069 */         rdd_mutableStateArray_0[0].setNullAt(0);
/* 070 */       } else {
/* 071 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);
/* 072 */       }
/* 073 */
/* 074 */       if (rdd_isNull_1) {
/* 075 */         rdd_mutableStateArray_0[0].setNullAt(1);
/* 076 */       } else {
/* 077 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);
/* 078 */       }
/* 079 */
/* 080 */       if (rdd_isNull_2) {
/* 081 */         rdd_mutableStateArray_0[0].setNullAt(2);
/* 082 */       } else {
/* 083 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);
/* 084 */       }
/* 085 */
/* 086 */       if (rdd_isNull_3) {
/* 087 */         rdd_mutableStateArray_0[0].setNullAt(3);
/* 088 */       } else {
/* 089 */         rdd_mutableStateArray_0[0].write(3, rdd_value_3);
/* 090 */       }
/* 091 */
/* 092 */       if (rdd_isNull_4) {
/* 093 */         rdd_mutableStateArray_0[0].setNullAt(4);
/* 094 */       } else {
/* 095 */         rdd_mutableStateArray_0[0].write(4, rdd_value_4);
/* 096 */       }
/* 097 */
/* 098 */       if (rdd_isNull_5) {
/* 099 */         rdd_mutableStateArray_0[0].setNullAt(5);
/* 100 */       } else {
/* 101 */         rdd_mutableStateArray_0[0].write(5, rdd_value_5);
/* 102 */       }
/* 103 */
/* 104 */       if (rdd_isNull_6) {
/* 105 */         rdd_mutableStateArray_0[0].setNullAt(6);
/* 106 */       } else {
/* 107 */         rdd_mutableStateArray_0[0].write(6, rdd_value_6);
/* 108 */       }
/* 109 */
/* 110 */       if (rdd_isNull_7) {
/* 111 */         rdd_mutableStateArray_0[0].setNullAt(7);
/* 112 */       } else {
/* 113 */         rdd_mutableStateArray_0[0].write(7, rdd_value_7);
/* 114 */       }
/* 115 */
/* 116 */       if (rdd_isNull_8) {
/* 117 */         rdd_mutableStateArray_0[0].setNullAt(8);
/* 118 */       } else {
/* 119 */         rdd_mutableStateArray_0[0].write(8, rdd_value_8);
/* 120 */       }
/* 121 */
/* 122 */       if (rdd_isNull_9) {
/* 123 */         rdd_mutableStateArray_0[0].setNullAt(9);
/* 124 */       } else {
/* 125 */         rdd_mutableStateArray_0[0].write(9, rdd_value_9);
/* 126 */       }
/* 127 */
/* 128 */       if (rdd_isNull_10) {
/* 129 */         rdd_mutableStateArray_0[0].setNullAt(10);
/* 130 */       } else {
/* 131 */         rdd_mutableStateArray_0[0].write(10, rdd_value_10);
/* 132 */       }
/* 133 */
/* 134 */       if (rdd_isNull_11) {
/* 135 */         rdd_mutableStateArray_0[0].setNullAt(11);
/* 136 */       } else {
/* 137 */         rdd_mutableStateArray_0[0].write(11, rdd_value_11);
/* 138 */       }
/* 139 */       append((rdd_mutableStateArray_0[0].getRow()));
/* 140 */       if (shouldStop()) return;
/* 141 */     }
/* 142 */   }
/* 143 */
/* 144 */ }

24/09/12 12:06:03 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( rdd_input_0.hasNext()) {
/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);
/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?
/* 030 */       null : (rdd_row_0.getUTF8String(0));
/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);
/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?
/* 033 */       null : (rdd_row_0.getUTF8String(1));
/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);
/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?
/* 036 */       null : (rdd_row_0.getUTF8String(2));
/* 037 */       boolean rdd_isNull_3 = rdd_row_0.isNullAt(3);
/* 038 */       UTF8String rdd_value_3 = rdd_isNull_3 ?
/* 039 */       null : (rdd_row_0.getUTF8String(3));
/* 040 */       boolean rdd_isNull_4 = rdd_row_0.isNullAt(4);
/* 041 */       UTF8String rdd_value_4 = rdd_isNull_4 ?
/* 042 */       null : (rdd_row_0.getUTF8String(4));
/* 043 */       boolean rdd_isNull_5 = rdd_row_0.isNullAt(5);
/* 044 */       UTF8String rdd_value_5 = rdd_isNull_5 ?
/* 045 */       null : (rdd_row_0.getUTF8String(5));
/* 046 */       boolean rdd_isNull_6 = rdd_row_0.isNullAt(6);
/* 047 */       UTF8String rdd_value_6 = rdd_isNull_6 ?
/* 048 */       null : (rdd_row_0.getUTF8String(6));
/* 049 */       boolean rdd_isNull_7 = rdd_row_0.isNullAt(7);
/* 050 */       UTF8String rdd_value_7 = rdd_isNull_7 ?
/* 051 */       null : (rdd_row_0.getUTF8String(7));
/* 052 */       boolean rdd_isNull_8 = rdd_row_0.isNullAt(8);
/* 053 */       UTF8String rdd_value_8 = rdd_isNull_8 ?
/* 054 */       null : (rdd_row_0.getUTF8String(8));
/* 055 */       boolean rdd_isNull_9 = rdd_row_0.isNullAt(9);
/* 056 */       UTF8String rdd_value_9 = rdd_isNull_9 ?
/* 057 */       null : (rdd_row_0.getUTF8String(9));
/* 058 */       boolean rdd_isNull_10 = rdd_row_0.isNullAt(10);
/* 059 */       UTF8String rdd_value_10 = rdd_isNull_10 ?
/* 060 */       null : (rdd_row_0.getUTF8String(10));
/* 061 */       boolean rdd_isNull_11 = rdd_row_0.isNullAt(11);
/* 062 */       UTF8String rdd_value_11 = rdd_isNull_11 ?
/* 063 */       null : (rdd_row_0.getUTF8String(11));
/* 064 */       rdd_mutableStateArray_0[0].reset();
/* 065 */
/* 066 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();
/* 067 */
/* 068 */       if (rdd_isNull_0) {
/* 069 */         rdd_mutableStateArray_0[0].setNullAt(0);
/* 070 */       } else {
/* 071 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);
/* 072 */       }
/* 073 */
/* 074 */       if (rdd_isNull_1) {
/* 075 */         rdd_mutableStateArray_0[0].setNullAt(1);
/* 076 */       } else {
/* 077 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);
/* 078 */       }
/* 079 */
/* 080 */       if (rdd_isNull_2) {
/* 081 */         rdd_mutableStateArray_0[0].setNullAt(2);
/* 082 */       } else {
/* 083 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);
/* 084 */       }
/* 085 */
/* 086 */       if (rdd_isNull_3) {
/* 087 */         rdd_mutableStateArray_0[0].setNullAt(3);
/* 088 */       } else {
/* 089 */         rdd_mutableStateArray_0[0].write(3, rdd_value_3);
/* 090 */       }
/* 091 */
/* 092 */       if (rdd_isNull_4) {
/* 093 */         rdd_mutableStateArray_0[0].setNullAt(4);
/* 094 */       } else {
/* 095 */         rdd_mutableStateArray_0[0].write(4, rdd_value_4);
/* 096 */       }
/* 097 */
/* 098 */       if (rdd_isNull_5) {
/* 099 */         rdd_mutableStateArray_0[0].setNullAt(5);
/* 100 */       } else {
/* 101 */         rdd_mutableStateArray_0[0].write(5, rdd_value_5);
/* 102 */       }
/* 103 */
/* 104 */       if (rdd_isNull_6) {
/* 105 */         rdd_mutableStateArray_0[0].setNullAt(6);
/* 106 */       } else {
/* 107 */         rdd_mutableStateArray_0[0].write(6, rdd_value_6);
/* 108 */       }
/* 109 */
/* 110 */       if (rdd_isNull_7) {
/* 111 */         rdd_mutableStateArray_0[0].setNullAt(7);
/* 112 */       } else {
/* 113 */         rdd_mutableStateArray_0[0].write(7, rdd_value_7);
/* 114 */       }
/* 115 */
/* 116 */       if (rdd_isNull_8) {
/* 117 */         rdd_mutableStateArray_0[0].setNullAt(8);
/* 118 */       } else {
/* 119 */         rdd_mutableStateArray_0[0].write(8, rdd_value_8);
/* 120 */       }
/* 121 */
/* 122 */       if (rdd_isNull_9) {
/* 123 */         rdd_mutableStateArray_0[0].setNullAt(9);
/* 124 */       } else {
/* 125 */         rdd_mutableStateArray_0[0].write(9, rdd_value_9);
/* 126 */       }
/* 127 */
/* 128 */       if (rdd_isNull_10) {
/* 129 */         rdd_mutableStateArray_0[0].setNullAt(10);
/* 130 */       } else {
/* 131 */         rdd_mutableStateArray_0[0].write(10, rdd_value_10);
/* 132 */       }
/* 133 */
/* 134 */       if (rdd_isNull_11) {
/* 135 */         rdd_mutableStateArray_0[0].setNullAt(11);
/* 136 */       } else {
/* 137 */         rdd_mutableStateArray_0[0].write(11, rdd_value_11);
/* 138 */       }
/* 139 */       append((rdd_mutableStateArray_0[0].getRow()));
/* 140 */       if (shouldStop()) return;
/* 141 */     }
/* 142 */   }
/* 143 */
/* 144 */ }

24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(78)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 78
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 78
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(75)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 75
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 75
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(82)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 82
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 82
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(95)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 95
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 95
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(47)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 47
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 47
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(62)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 62
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 62
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(43)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 43
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 43
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(48)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 48
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 48
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(85)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 85
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 85
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(84)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 84
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 84
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(67)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 67
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 67
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(88)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 88
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 88
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(68)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 68
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 68
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(61)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 61
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 61
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(63)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 63
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 63
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(60)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 60
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 60
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(46)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 46
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 46
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(71)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 71
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 71
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(89)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 89
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 89
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(96)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 96
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 96
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(42)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 42
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 42
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(70)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 70
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 70
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(73)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 73
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 73
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(83)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 83
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 83
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(64)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 64
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 64
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(93)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 93
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 93
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(41)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 41
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 41
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(86)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 86
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 86
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(79)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 79
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 79
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(90)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 90
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 90
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(55)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 55
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 55
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(81)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 81
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 81
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(59)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 59
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 59
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(52)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 52
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 52
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(51)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 51
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 51
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(56)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 56
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 56
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(66)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 66
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 66
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(45)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 45
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 45
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(69)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 69
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 69
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(53)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 53
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 53
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(50)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 50
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 50
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(1)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning broadcast 1
24/09/12 12:06:03 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 1
24/09/12 12:06:03 DEBUG BlockManagerStorageEndpoint: removing broadcast 1
24/09/12 12:06:03 DEBUG BlockManager: Removing broadcast 1
24/09/12 12:06:03 DEBUG BlockManager: Removing block broadcast_1
24/09/12 12:06:03 DEBUG MemoryStore: Block broadcast_1 of size 48264 dropped from memory (free 455484212)
24/09/12 12:06:03 DEBUG BlockManager: Removing block broadcast_1_piece0
24/09/12 12:06:03 DEBUG MemoryStore: Block broadcast_1_piece0 of size 17202 dropped from memory (free 455501414)
24/09/12 12:06:03 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:06:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c8faee0cb5df:43645 in memory (size: 16.8 KiB, free: 434.4 MiB)
24/09/12 12:06:03 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
24/09/12 12:06:03 DEBUG BlockManager: Told master about block broadcast_1_piece0
24/09/12 12:06:03 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(0, 172.29.0.6, 40323, None)
24/09/12 12:06:03 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 1, response is 0
24/09/12 12:06:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.29.0.6:40323 in memory (size: 16.8 KiB, free: 434.4 MiB)
24/09/12 12:06:03 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to c8faee0cb5df:38537
24/09/12 12:06:03 INFO CodeGenerator: Code generated in 16.306083 ms
24/09/12 12:06:03 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted
24/09/12 12:06:03 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned broadcast 1
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(58)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 58
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 58
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(57)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 57
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 57
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(44)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 44
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 44
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(80)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 80
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 80
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(92)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 92
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 92
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(40)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 40
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 40
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(72)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 72
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 72
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(87)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 87
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 87
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(74)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 74
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 74
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(77)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 77
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 77
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(76)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 76
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 76
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(94)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 94
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 94
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(49)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 49
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 49
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(65)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 65
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 65
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(54)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 54
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 54
24/09/12 12:06:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(91)
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaning accumulator 91
24/09/12 12:06:03 DEBUG ContextCleaner: Cleaned accumulator 91
24/09/12 12:06:03 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2
24/09/12 12:06:03 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
24/09/12 12:06:03 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
24/09/12 12:06:03 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
24/09/12 12:06:03 INFO SparkContext: Starting job: call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
24/09/12 12:06:03 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 10 took 0.000087 seconds
24/09/12 12:06:03 DEBUG DAGScheduler: Merging stage rdd profiles: Set()
24/09/12 12:06:03 INFO DAGScheduler: Got job 2 (call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
24/09/12 12:06:03 INFO DAGScheduler: Final stage: ResultStage 2 (call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
24/09/12 12:06:03 INFO DAGScheduler: Parents of final stage: List()
24/09/12 12:06:03 INFO DAGScheduler: Missing parents: List()
24/09/12 12:06:03 DEBUG DAGScheduler: submitStage(ResultStage 2 (name=call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617;jobs=2))
24/09/12 12:06:03 DEBUG DAGScheduler: missing: List()
24/09/12 12:06:03 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
24/09/12 12:06:03 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)
24/09/12 12:06:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 46.3 KiB, free 434.4 MiB)
24/09/12 12:06:03 DEBUG BlockManager: Put block broadcast_2 locally took 0 ms
24/09/12 12:06:03 DEBUG BlockManager: Putting block broadcast_2 without replication took 0 ms
24/09/12 12:06:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 434.3 MiB)
24/09/12 12:06:03 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, c8faee0cb5df, 43645, None)
24/09/12 12:06:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c8faee0cb5df:43645 (size: 16.6 KiB, free: 434.4 MiB)
24/09/12 12:06:03 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0
24/09/12 12:06:03 DEBUG BlockManager: Told master about block broadcast_2_piece0
24/09/12 12:06:03 DEBUG BlockManager: Put block broadcast_2_piece0 locally took 1 ms
24/09/12 12:06:03 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took 1 ms
24/09/12 12:06:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/09/12 12:06:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
24/09/12 12:06:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/09/12 12:06:03 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0
24/09/12 12:06:03 DEBUG TaskSetManager: Adding pending tasks took 0 ms
24/09/12 12:06:03 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NO_PREF, ANY
24/09/12 12:06:03 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0
24/09/12 12:06:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.29.0.6, executor 0, partition 0, PROCESS_LOCAL, 15039 bytes) 
24/09/12 12:06:03 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
24/09/12 12:06:03 DEBUG StandaloneSchedulerBackend$StandaloneDriverEndpoint: Launching task 2 on executor id: 0 hostname: 172.29.0.6.
24/09/12 12:06:03 DEBUG BlockManager: Getting local block broadcast_2_piece0 as bytes
24/09/12 12:06:03 DEBUG BlockManager: Level for block broadcast_2_piece0 is StorageLevel(disk, memory, 1 replicas)
24/09/12 12:06:03 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(0, 172.29.0.6, 40323, None)
24/09/12 12:06:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.29.0.6:40323 (size: 16.6 KiB, free: 434.4 MiB)
24/09/12 12:06:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 596 ms on 172.29.0.6 (executor 0) (1/1)
24/09/12 12:06:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/09/12 12:06:04 INFO DAGScheduler: ResultStage 2 (call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.608 s
24/09/12 12:06:04 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 0
24/09/12 12:06:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/12 12:06:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/09/12 12:06:04 INFO DAGScheduler: Job 2 finished: call at /opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.611053 s
2024-09-12 12:06:04,357 - DEBUG - Answer received: !yto61
2024-09-12 12:06:04,357 - DEBUG - Command to send: c
o20
setCallSite
n
e

2024-09-12 12:06:04,357 - DEBUG - Answer received: !yv
2024-09-12 12:06:04,357 - DEBUG - Command to send: a
e
o61
e

2024-09-12 12:06:04,358 - DEBUG - Answer received: !yi3
2024-09-12 12:06:04,358 - DEBUG - Command to send: a
g
o61
i0
e

2024-09-12 12:06:04,358 - DEBUG - Answer received: !yi35937
2024-09-12 12:06:04,358 - DEBUG - Command to send: a
e
o61
e

2024-09-12 12:06:04,358 - DEBUG - Answer received: !yi3
2024-09-12 12:06:04,358 - DEBUG - Command to send: a
g
o61
i1
e

2024-09-12 12:06:04,359 - DEBUG - Answer received: !ys5936ff84dc361881bab3bb2048ceabca7cb6c0b87ceb649c15876b5bbdf45f4b
2024-09-12 12:06:04,366 - INFO - Inserting data: {'id': 'test1', 'first_name': 'John', 'last_name': 'Doe', 'gender': None, 'address': None, 'post_code': None, 'email': None, 'username': None, 'dob': None, 'registered_date': None, 'phone': None, 'picture': None}
2024-09-12 12:06:04,366 - ERROR - could not insert data: badly formed hexadecimal UUID string
Traceback (most recent call last):
  File "/app/spark_stream.py", line 64, in insert_data
    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__
    raise ValueError('badly formed hexadecimal UUID string')
ValueError: badly formed hexadecimal UUID string
2024-09-12 12:06:04,367 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/spark_stream.py", line 136, in process_batch
    insert_data(cassandra_conn, **data)
  File "/app/spark_stream.py", line 64, in insert_data
    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__
    raise ValueError('badly formed hexadecimal UUID string')
ValueError: badly formed hexadecimal UUID string
24/09/12 12:06:04 ERROR MicroBatchExecution: Query [id = 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, runId = 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/spark_stream.py", line 136, in process_batch
    insert_data(cassandra_conn, **data)
  File "/app/spark_stream.py", line 64, in insert_data
    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__
    raise ValueError('badly formed hexadecimal UUID string')
ValueError: badly formed hexadecimal UUID string

	at py4j.Protocol.getReturnValue(Protocol.java:476)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
24/09/12 12:06:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, runId = 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d] has been shutdown
24/09/12 12:06:04 DEBUG StateStoreCoordinator: Deactivating instances related to checkpoint location 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d: 
2024-09-12 12:06:04,436 - DEBUG - Answer received: !xro62
2024-09-12 12:06:04,436 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,437 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,437 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,437 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,437 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,438 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,438 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,438 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,438 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro62
e

2024-09-12 12:06:04,439 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,439 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,440 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,440 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,440 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,440 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,440 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,440 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,440 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,440 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro62
e

2024-09-12 12:06:04,441 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,441 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,442 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,442 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,442 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,442 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,442 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,442 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,442 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,442 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro62
e

2024-09-12 12:06:04,443 - DEBUG - Answer received: !ybtrue
2024-09-12 12:06:04,443 - DEBUG - Command to send: c
o62
getMessage
e

2024-09-12 12:06:04,443 - DEBUG - Answer received: !ys[STREAM_FAILED] Query [id = 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, runId = 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call\n    raise e\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File "/app/spark_stream.py", line 136, in process_batch\n    insert_data(cassandra_conn, **data)\n  File "/app/spark_stream.py", line 64, in insert_data\n    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__\n    raise ValueError('badly formed hexadecimal UUID string')\nValueError: badly formed hexadecimal UUID string\n
2024-09-12 12:06:04,443 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:06:04,444 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,444 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:06:04,445 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,445 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:06:04,445 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,445 - DEBUG - Command to send: r
u
org.apache.spark.util
rj
e

2024-09-12 12:06:04,445 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,445 - DEBUG - Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-09-12 12:06:04,446 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils
2024-09-12 12:06:04,446 - DEBUG - Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-09-12 12:06:04,446 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,446 - DEBUG - Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro62
e

2024-09-12 12:06:04,447 - DEBUG - Answer received: !ysorg.apache.spark.sql.streaming.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call\n    raise e\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File "/app/spark_stream.py", line 136, in process_batch\n    insert_data(cassandra_conn, **data)\n  File "/app/spark_stream.py", line 64, in insert_data\n    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__\n    raise ValueError('badly formed hexadecimal UUID string')\nValueError: badly formed hexadecimal UUID string\n\n=== Streaming Query ===\nIdentifier: [id = 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, runId = 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d]\nCurrent Committed Offsets: {KafkaV2[Subscribe[user_data]]: {"user_data":{"0":0}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[user_data]]: {"user_data":{"0":1}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSourceV1 ForeachBatchSink, 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, [checkpointLocation=/tmp/checkpoint], Append\n+- Project [data#23.id AS id#25, data#23.first_name AS first_name#26, data#23.last_name AS last_name#27, data#23.gender AS gender#28, data#23.address AS address#29, data#23.post_code AS post_code#30, data#23.email AS email#31, data#23.username AS username#32, data#23.dob AS dob#33, data#23.registered_date AS registered_date#34, data#23.phone AS phone#35, data#23.picture AS picture#36]\n   +- Project [from_json(StructField(id,StringType,true), StructField(first_name,StringType,true), StructField(last_name,StringType,true), StructField(gender,StringType,true), StructField(address,StringType,true), StructField(post_code,StringType,true), StructField(email,StringType,true), StructField(username,StringType,true), StructField(dob,StringType,true), StructField(registered_date,StringType,true), StructField(phone,StringType,true), StructField(picture,StringType,true), json#21, Some(Etc/UTC)) AS data#23]\n      +- Project [cast(value#8 as string) AS json#21]\n         +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2617bb4c, KafkaV2[Subscribe[user_data]]\n\n	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:332)\n	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\nCaused by: py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call\n    raise e\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File "/app/spark_stream.py", line 136, in process_batch\n    insert_data(cassandra_conn, **data)\n  File "/app/spark_stream.py", line 64, in insert_data\n    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__\n    raise ValueError('badly formed hexadecimal UUID string')\nValueError: badly formed hexadecimal UUID string\n\n	at py4j.Protocol.getReturnValue(Protocol.java:476)\n	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)\n	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n	... 4 more\n
2024-09-12 12:06:04,447 - DEBUG - Command to send: c
o62
getCause
e

2024-09-12 12:06:04,447 - DEBUG - Answer received: !yro63
2024-09-12 12:06:04,447 - DEBUG - Command to send: c
o62
getCause
e

2024-09-12 12:06:04,447 - DEBUG - Answer received: !yro64
2024-09-12 12:06:04,447 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,457 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,458 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,458 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,458 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,461 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,461 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,467 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,467 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro64
e

2024-09-12 12:06:04,467 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,467 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,483 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,484 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,484 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,484 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,484 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,484 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,484 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,484 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro64
e

2024-09-12 12:06:04,484 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,484 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,487 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,487 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,487 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,487 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,492 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,492 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,492 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,492 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro64
e

2024-09-12 12:06:04,493 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,493 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,497 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,497 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,500 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,503 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,503 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,503 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,503 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,503 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro64
e

2024-09-12 12:06:04,503 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,504 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,515 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,515 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,516 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,516 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,516 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,516 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,516 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,517 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro64
e

2024-09-12 12:06:04,517 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,517 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,519 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,519 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,519 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,519 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,519 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,519 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,520 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,520 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro64
e

2024-09-12 12:06:04,520 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,520 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,521 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,521 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,522 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,522 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,522 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,522 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,522 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,522 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro64
e

2024-09-12 12:06:04,523 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,523 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,524 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,524 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,525 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,525 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,525 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,525 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,526 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,526 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro64
e

2024-09-12 12:06:04,526 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,526 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,528 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,528 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,528 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,528 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,528 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,528 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,528 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,528 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro64
e

2024-09-12 12:06:04,529 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,529 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,530 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,530 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,530 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,530 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,531 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,531 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,531 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,531 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro64
e

2024-09-12 12:06:04,532 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,532 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,533 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,533 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,533 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,533 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,533 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,533 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,533 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,533 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro64
e

2024-09-12 12:06:04,534 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,534 - DEBUG - Command to send: r
u
py4j
rj
e

2024-09-12 12:06:04,535 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,535 - DEBUG - Command to send: r
u
py4j.reflection
rj
e

2024-09-12 12:06:04,535 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,535 - DEBUG - Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-09-12 12:06:04,535 - DEBUG - Answer received: !ycpy4j.reflection.TypeUtil
2024-09-12 12:06:04,535 - DEBUG - Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-09-12 12:06:04,535 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,535 - DEBUG - Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro64
e

2024-09-12 12:06:04,536 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,536 - DEBUG - Command to send: c
o64
getCause
e

2024-09-12 12:06:04,536 - DEBUG - Answer received: !yn
2024-09-12 12:06:04,536 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:06:04,537 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,537 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:06:04,537 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,537 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:06:04,538 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,538 - DEBUG - Command to send: r
u
org.apache.spark.util
rj
e

2024-09-12 12:06:04,538 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,538 - DEBUG - Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-09-12 12:06:04,538 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils
2024-09-12 12:06:04,538 - DEBUG - Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-09-12 12:06:04,539 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,539 - DEBUG - Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro64
e

2024-09-12 12:06:04,539 - DEBUG - Answer received: !yspy4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call\n    raise e\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File "/app/spark_stream.py", line 136, in process_batch\n    insert_data(cassandra_conn, **data)\n  File "/app/spark_stream.py", line 64, in insert_data\n    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__\n    raise ValueError('badly formed hexadecimal UUID string')\nValueError: badly formed hexadecimal UUID string\n\n	at py4j.Protocol.getReturnValue(Protocol.java:476)\n	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)\n	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n
2024-09-12 12:06:04,539 - DEBUG - Command to send: c
o64
toString
e

2024-09-12 12:06:04,540 - DEBUG - Answer received: !yspy4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call\n    raise e\n  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File "/app/spark_stream.py", line 136, in process_batch\n    insert_data(cassandra_conn, **data)\n  File "/app/spark_stream.py", line 64, in insert_data\n    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__\n    raise ValueError('badly formed hexadecimal UUID string')\nValueError: badly formed hexadecimal UUID string\n
2024-09-12 12:06:04,540 - ERROR - An error occurred in the main execution:
2024-09-12 12:06:04,541 - DEBUG - Command to send: r
u
org
rj
e

2024-09-12 12:06:04,542 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,542 - DEBUG - Command to send: r
u
org.apache
rj
e

2024-09-12 12:06:04,543 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,543 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2024-09-12 12:06:04,543 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,543 - DEBUG - Command to send: r
u
org.apache.spark.sql
rj
e

2024-09-12 12:06:04,543 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,543 - DEBUG - Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-09-12 12:06:04,544 - DEBUG - Answer received: !yp
2024-09-12 12:06:04,544 - DEBUG - Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-09-12 12:06:04,544 - DEBUG - Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-09-12 12:06:04,544 - DEBUG - Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-09-12 12:06:04,545 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,545 - DEBUG - Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-09-12 12:06:04,546 - DEBUG - Answer received: !yro65
2024-09-12 12:06:04,547 - DEBUG - Command to send: c
o65
pysparkJVMStacktraceEnabled
e

2024-09-12 12:06:04,547 - DEBUG - Answer received: !ybfalse
2024-09-12 12:06:04,548 - ERROR - Traceback (most recent call last):
  File "/app/spark_stream.py", line 170, in <module>
    streaming_query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
    return self._jsq.awaitTermination()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 70dc2991-e1a3-4dac-8b5e-fd01a10c8750, runId = 8bdbe83c-ee41-4682-92cb-bb2e8ebff89d] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/spark_stream.py", line 136, in process_batch
    insert_data(cassandra_conn, **data)
  File "/app/spark_stream.py", line 64, in insert_data
    user_id = UUID(kwargs.get('id')) if kwargs.get('id') else uuid.uuid4()
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/python/lib/python3.12/uuid.py", line 178, in __init__
    raise ValueError('badly formed hexadecimal UUID string')
ValueError: badly formed hexadecimal UUID string


2024-09-12 12:06:04,548 - INFO - Stopping Spark session
2024-09-12 12:06:04,548 - DEBUG - Command to send: c
o20
stop
e

24/09/12 12:06:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/09/12 12:06:04 INFO SparkUI: Stopped Spark web UI at http://c8faee0cb5df:4040
24/09/12 12:06:04 INFO StandaloneSchedulerBackend: Shutting down all executors
24/09/12 12:06:04 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
24/09/12 12:06:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/09/12 12:06:04 INFO MemoryStore: MemoryStore cleared
24/09/12 12:06:04 INFO BlockManager: BlockManager stopped
24/09/12 12:06:04 INFO BlockManagerMaster: BlockManagerMaster stopped
24/09/12 12:06:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/09/12 12:06:04 DEBUG PoolThreadCache: Freed 3 thread-local buffer(s) from thread: rpc-server-4-1
24/09/12 12:06:04 INFO SparkContext: Successfully stopped SparkContext
2024-09-12 12:06:04,611 - DEBUG - Answer received: !yv
2024-09-12 12:06:04,822 - DEBUG - Command to send: r
u
SparkSession
rj
e

2024-09-12 12:06:04,827 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2024-09-12 12:06:04,827 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
clearDefaultSession
e

2024-09-12 12:06:04,829 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,829 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
clearDefaultSession
e

2024-09-12 12:06:04,830 - DEBUG - Answer received: !yv
2024-09-12 12:06:04,830 - DEBUG - Command to send: r
u
SparkSession
rj
e

2024-09-12 12:06:04,832 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2024-09-12 12:06:04,832 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
clearActiveSession
e

2024-09-12 12:06:04,833 - DEBUG - Answer received: !ym
2024-09-12 12:06:04,833 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
clearActiveSession
e

2024-09-12 12:06:04,833 - DEBUG - Answer received: !yv
2024-09-12 12:06:04,834 - INFO - Closing Cassandra connection
2024-09-12 12:06:04,834 - DEBUG - Closing connection (281472799621728) to 172.29.0.3:9042
2024-09-12 12:06:04,834 - DEBUG - Closed socket to 172.29.0.3:9042
2024-09-12 12:06:04,834 - INFO - Spark Streaming application stopped
2024-09-12 12:06:04,834 - DEBUG - Shutting down Cluster Scheduler
2024-09-12 12:06:04,835 - DEBUG - Shutting down control connection
2024-09-12 12:06:04,835 - DEBUG - Closing connection (281472827670352) to 172.29.0.3:9042
2024-09-12 12:06:04,835 - DEBUG - Closed socket to 172.29.0.3:9042
2024-09-12 12:06:04,835 - INFO - Closing down clientserver connection
24/09/12 12:06:04 INFO ShutdownHookManager: Shutdown hook called
24/09/12 12:06:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-b30ce0bb-f112-4cd2-b999-1685527b89bf
24/09/12 12:06:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90
24/09/12 12:06:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-84e9b6a7-23db-4dc2-86ba-fe23d81fbb90/pyspark-4379998f-7d97-4cbc-ab7d-ba1212cc9b04
24/09/12 12:06:04 DEBUG FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.FilterFileSystem.close(FilterFileSystem.java:529)); Key: (spark (auth:SIMPLE))@file://; URI: file:///; Object Identity Hash: 33790f6e
24/09/12 12:06:04 DEBUG FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.RawLocalFileSystem.close(RawLocalFileSystem.java:759)); Key: null; URI: file:///; Object Identity Hash: 6affb373
24/09/12 12:06:04 DEBUG ShutdownHookManager: Completed shutdown in 0.009 seconds; Timeouts: 0
24/09/12 12:06:04 DEBUG ShutdownHookManager: ShutdownHookManager completed shutdown.
